---
topic_name: Research – Single-Subject and Group Designs
domain: 7: Research Methods & Statistics
slug: research-single-subject-and-group-designs
generated_at: 2025-11-15T21:30:53.628Z
model: claude-sonnet-4-5-20250929
version: 1
---

## Why Research Design Matters More Than You Think

You're studying for the EPPP, and you might be tempted to skim through research methods. After all, you're planning to be a therapist, not a researcher, right? Here's the thing: understanding research design is like knowing how to check if a car is safe before you drive it. Every treatment you'll recommend, every assessment you'll use, every clinical decision you'll make is based on research findings. If you can't evaluate how that research was conducted, you're essentially trusting someone else's word without looking under the hood.

This lesson breaks down single-subject and group research designs—the tools researchers use to answer questions about what works, what doesn't, and why. Think of these designs as different types of cameras: some give you a close-up portrait of one person over time, others capture a wide-angle shot of many people at once. Each has its purpose, and knowing which one to use (or evaluate) is crucial for your practice and your exam.

## The Big Picture: Qualitative vs. Quantitative Research

Before diving into specific designs, let's clarify the fundamental split in research approaches. It's similar to how you might gather information about a new restaurant: you could read detailed reviews describing the atmosphere and experience (qualitative), or you could check the star rating and see that 87% of people recommended it (quantitative).

**Qualitative research** explores the "what it's like" aspect of human experience. It's narrative, interpretive, and focuses on depth rather than numbers. Four main approaches exist:

- **Grounded theory**: Researchers interview and observe people to build a theory from the ground up. Imagine interviewing dozens of people who successfully quit smoking and discovering patterns in their stories that form a new theory about behavior change.

- **Phenomenology**: This digs deep into lived experience through intensive interviews. If you wanted to understand what it's really like to live with social anxiety, you'd sit with people and explore every angle of their experience.

- **Ethnography**: Researchers immerse themselves in a culture or setting, like joining a support group for months to understand its dynamics from the inside.

- **Thematic analysis**: This method identifies patterns across interviews or focus groups, like noticing that everyone who struggled with therapy mentioned feeling misunderstood by their therapist.

**Triangulation** is a quality-check method, most common in qualitative research. It's like when you're trying to understand a friend's relationship problems—you don't just listen to their version. You might also consider what mutual friends observe, what happened at different times, and different perspectives on the situation. This gives you a more complete, trustworthy picture.

## Quantitative Research: When Numbers Tell the Story

Quantitative research measures things numerically. It's organized into three categories: descriptive, correlational, and experimental.

### Descriptive Research

This type answers "what's happening?" without trying to explain why. Surveys, case studies, and observational studies fall here.

**Observational methods** come in two flavors:

| Method | Best Used For | Example |
|--------|---------------|---------|
| **Interval Recording** | Frequent behaviors without clear start/stop | Recording whether a child is paying attention during 30 two-minute intervals of class time |
| **Event Recording** | Infrequent behaviors with clear start/stop | Counting how many times a student leaves their seat during a 45-minute period |

Think of interval recording like checking your phone periodically to see if you have notifications, while event recording is like counting every single text message you receive.

### Correlational Research

This examines relationships between variables. When researchers want to know if two things are connected—like sleep quality and mood, or childhood trauma and adult anxiety—they use correlational designs. The key limitation: correlation doesn't prove causation. Just because people who exercise more report less depression doesn't mean exercise causes the improvement (maybe less depressed people simply have more energy to exercise).

### Experimental Research

This is the gold standard for establishing cause and effect. Experimental research comes in two types: **true experimental** (with random assignment) and **quasi-experimental** (without random assignment).

Random assignment is like shuffling a deck of cards before dealing them into two piles. You're increasing the odds that both piles are similar. Without it, you might unknowingly stack the deck—maybe all the motivated people end up in one group.

## Single-Subject Designs: The Close-Up Portrait

Single-subject designs focus intensely on one person (or a small group treated as one unit) over time. They're incredibly practical for clinicians because they mirror what you actually do in therapy: work with an individual, track their progress, and adjust your approach based on what's working.

All single-subject designs share key features:
- A baseline phase (A) where you measure the problem without intervention
- A treatment phase (B) where you apply your intervention
- Multiple measurements during each phase
- Waiting for stable baseline performance before starting treatment

### The AB Design

This is the simplest design: measure the problem, then apply treatment and keep measuring. It's like tracking your mood for two weeks without changing anything, then starting a new meditation practice and continuing to track your mood.

**The problem**: If your mood improves after you start meditating, was it the meditation or something else that happened around the same time (like finally resolving that conflict with your roommate)? This is the history threat—when other events coincide with your intervention.

### Reversal (ABAB) Designs

The ABAB design adds sophistication: baseline, treatment, remove treatment (second baseline), apply treatment again. If the behavior improves during both treatment phases and worsens during the second baseline, you've got strong evidence that your treatment caused the change.

It's like testing whether a new medication helps your migraines: headaches at baseline, fewer headaches on medication, more headaches when you stop, fewer headaches when you restart. That pattern strongly suggests the medication works.

**The ethical dilemma**: Sometimes you can't ethically remove an effective treatment. If you've helped someone reduce self-harm behaviors, you wouldn't withdraw treatment just to prove it was working.

### Multiple Baseline Design

This elegant solution addresses the reversal design's limitations. Instead of withdrawing treatment, you apply it sequentially across different behaviors, settings, or people.

Imagine you're working with a teenager who shows three aggressive behaviors at school: cursing, throwing objects, and pushing peers. You'd use this approach:

1. Measure all three behaviors for a week (all in baseline)
2. Apply your intervention to cursing only, still measuring all three
3. Add intervention for cursing AND throwing, still measuring all three
4. Finally, apply intervention to all three behaviors

If each behavior only improves when you specifically target it, you've demonstrated that your intervention—not just the passage of time or other factors—caused the changes.

The beauty here: no withdrawal needed. Once you intervene on a behavior, you keep intervening.

## Group Designs: The Wide-Angle Shot

Group designs study multiple people simultaneously. They come in three categories based on how subjects experience the independent variable.

### Between-Subjects Designs

Each group gets a different treatment. Three groups comparing three different therapy approaches? That's between-subjects. It's like three friends each trying a different dating app to see which one works best—each person only uses one app.

**Randomized Controlled Trials (RCTs)** are the superstars here. They're considered the gold standard because random assignment to groups maximizes internal validity (confidence that your treatment caused the results). However, RCTs have strict inclusion criteria and controlled conditions, which can limit external validity—how well the results apply to real-world settings.

Think of it this way: an RCT is like testing a new phone in a pristine laboratory with perfect WiFi and controlled temperature. You'll get precise data about the phone's capabilities, but that tells you less about how it'll perform when you're using it on a crowded subway with spotty service and freezing hands.

### Within-Subjects Designs

Every participant experiences all levels of the independent variable at different times. Testing three medication doses? Each person tries all three doses sequentially.

The **time-series design** is essentially a group version of the AB design. You measure everyone repeatedly before and after an intervention—like tracking a company's employee satisfaction scores monthly for six months before and six months after implementing a new wellness program.

Advantages: You need fewer participants because everyone experiences all conditions. Disadvantages: order effects (maybe the first treatment influences how later treatments work) and time commitment.

### Mixed Designs

These combine between-subjects and within-subjects variables. For example: three groups receive different medication doses (between-subjects), and you measure everyone's symptoms weekly for six weeks (within-subjects).

This is like studying three different workout programs where participants are assigned to one program (between-subjects) but you track their fitness levels every week for three months (within-subjects).

## Factorial Designs: Understanding Interactions

A factorial design includes two or more independent variables, letting you examine both **main effects** (the impact of each variable individually) and **interaction effects** (how variables combine to produce effects).

Consider a study examining therapy type (cognitive-behavioral vs. psychodynamic) and medication (antidepressant vs. placebo) for depression. You might find:

- **Main effect of therapy**: CBT works better than psychodynamic therapy overall
- **Main effect of medication**: Antidepressants work better than placebo overall
- **Interaction effect**: CBT works equally well with or without medication, but psychodynamic therapy only works well when combined with medication

That interaction is crucial. It tells you that the "best" treatment depends on which therapy you're using. Without a factorial design, you'd miss this nuance.

Think of it like coffee and sleep: caffeine might boost your alertness (main effect), and adequate sleep also boosts alertness (main effect), but caffeine's impact might be much stronger when you're sleep-deprived than when you're well-rested (interaction effect).

## Analogue Research: The Controlled Approximation

Analogue research uses situations that approximate real life but aren't identical—like using college students instead of clinical populations, or laboratory settings instead of natural environments.

It's a trade-off: you gain internal validity (control over extraneous variables) but lose external validity (generalizability). Testing a social anxiety intervention in a quiet lab with compliant college students might show promising results that don't translate to anxious adults in chaotic real-world situations.

## Developmental Research: Tracking Changes Over Time

### Longitudinal Research

Following the same people over years or decades. Want to know if childhood attachment style predicts adult relationship success? Track the same individuals from age 5 to age 35.

**Pros**: You can see actual developmental changes within individuals.

**Cons**: Expensive, time-consuming, and vulnerable to attrition bias (people who drop out might differ systematically from those who stay, skewing your results).

### Cross-Sectional Research

Comparing different age groups at one point in time. Want to study how memory changes with age? Test 30-year-olds, 50-year-olds, and 70-year-olds all this year.

**Pros**: Quick and relatively inexpensive.

**Cons**: Cohort effects—different age groups grew up in different eras. Are 70-year-olds' lower scores on technology tasks due to aging or because they didn't grow up using computers?

### Cross-Sequential Research

Combines both approaches: study multiple age groups and follow them over time. Test people who are currently 30, 40, and 50, then test them again in 10 and 20 years.

This helps separate age effects from cohort effects, but it's still expensive and time-consuming.

| Design | Time Investment | Cost | Cohort Effects | Attrition Bias |
|--------|----------------|------|----------------|----------------|
| Longitudinal | High | High | No | Yes |
| Cross-Sectional | Low | Low | Yes | No |
| Cross-Sequential | Medium | Medium | Partially controlled | Yes |

## Sampling: Who's in Your Study?

### Probability Sampling

Random selection where everyone in the population has an equal chance of being chosen. Four main methods:

1. **Simple random sampling**: Like drawing names from a hat containing everyone's name
2. **Systematic random sampling**: Selecting every 10th person from a list
3. **Stratified random sampling**: Dividing the population into relevant subgroups (age, gender, diagnosis) and randomly sampling from each
4. **Cluster random sampling**: Randomly selecting entire groups (like school districts) when the population is too large for individual sampling

Probability sampling maximizes representativeness but can still have **sampling error**—random chance differences between your sample and the population, especially with small samples.

### Non-Probability Sampling

Non-random selection where not everyone has an equal chance of being chosen. This risks **sampling bias**—systematic over- or under-representation of certain characteristics.

Methods include:
- **Convenience sampling**: Using whoever's available (like surveying students in your classes)
- **Voluntary response sampling**: Including only volunteers
- **Purposive sampling**: Deliberately selecting specific types of people
- **Snowball sampling**: Having participants recruit other participants

These methods are useful for exploratory research but problematic for hypothesis testing.

## Community-Based Participatory Research: Research as Partnership

CBPR treats community members as equal partners in all research phases. Instead of researchers studying a community from the outside, they collaborate with community members to identify problems, design studies, collect data, and implement solutions.

It's like the difference between a manager deciding what changes to make versus employees and management working together to identify problems and co-create solutions. The latter approach produces more relevant, sustainable, and culturally appropriate outcomes.

## Common Misconceptions

**"Single-subject designs aren't real research"**: False. They provide rigorous evidence of treatment effectiveness for individuals, which is highly relevant to clinical practice.

**"You always need random assignment for good research"**: Not true. While random assignment strengthens causal conclusions, quasi-experimental and non-experimental designs answer important questions that true experiments can't address ethically or practically.

**"Bigger samples are always better"**: Not necessarily. A well-designed study with 30 participants can be more informative than a poorly designed study with 300.

**"Correlation proves causation if the relationship is strong enough"**: Never. Correlation never proves causation, regardless of strength. You need experimental manipulation to establish causality.

**"Qualitative research is less rigorous than quantitative research"**: Wrong. Qualitative research has different standards of rigor, but it's not inherently less rigorous. Triangulation and other quality-check methods ensure trustworthy findings.

## Practice Tips for Remembering

**For single-subject designs**: Think alphabetically. AB is simplest. ABAB adds sophistication. Multiple baseline spreads across—Multiple Behaviors, Multiple Settings, Multiple Subjects.

**For group designs**: Between = separate groups (be-TWEEN). Within = same people (with-IN). Mixed = both.

**For sampling**: Probability starts with P, like "Perfect chance"—everyone has an equal shot. Non-probability lacks this.

**For developmental designs**: Longitudinal = LONG time, same people. Cross-sectional = CROSS sections of ages, one time. Cross-sequential = BOTH.

**For validity trade-offs**: Lab settings = high internal validity (controlled) but lower external validity (less generalizable). Natural settings = the reverse.

## Key Takeaways

- **Research designs are tools**: Each answers different questions with different strengths and limitations
- **Single-subject designs** track individuals intensively over time; perfect for clinical work
- **AB designs** are simple but can't rule out history effects
- **ABAB designs** provide stronger evidence by withdrawing and reapplying treatment
- **Multiple baseline designs** avoid withdrawal problems by staggering intervention across behaviors, settings, or people
- **Between-subjects designs** compare different groups; RCTs use random assignment for causal conclusions
- **Within-subjects designs** give everyone all conditions; more efficient but risk order effects
- **Mixed designs** combine between and within variables
- **Factorial designs** reveal interactions between variables, not just main effects
- **Longitudinal research** follows the same people over time; cross-sectional compares different ages once
- **Random sampling** increases representativeness; random assignment increases internal validity
- **CBPR** makes communities equal research partners, improving relevance and sustainability
- **The validity trade-off**: More control typically means less generalizability and vice versa

Understanding these designs isn't just about passing the EPPP—it's about becoming a critical consumer of research who can evaluate whether a "proven" treatment is actually worth recommending to your clients.