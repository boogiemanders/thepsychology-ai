---
topic_name: Item Analysis and Test Reliability
domain: 5: Test Construction
slug: item-analysis-and-test-reliability
generated_at: 2025-11-16T16:18:57.735Z
version: 3
---

## Why Item Analysis and Test Reliability Matter for Your Psychology Career

You've probably taken dozens of tests in your life, from college exams to professional assessments. But have you ever wondered what makes a good test actually good? When you're sitting across from a client, interpreting their depression inventory scores, or reviewing cognitive assessment results, you need to trust that those numbers mean something real and consistent.

That's where item analysis and test reliability come in. These concepts aren't just abstract statistics. They're the foundation that determines whether the tests you use in practice are worth the paper they're printed on. Understanding these principles will help you choose appropriate assessments, interpret scores accurately, and explain results to clients with confidence.

## Classical Test Theory: The Foundation

Let's start with the basic framework that underlies most psychological testing: Classical Test Theory (CTT). The core idea is simple but profound. Every score someone gets on a test (we call this the "obtained score") is made up of two parts:

**Obtained Score = True Score + Measurement Error**

The **true score** represents the person's actual level of whatever you're measuring. Their real intelligence, their genuine depression level, their authentic personality traits. {{M}}Think of it like your actual skill at cooking. You know roughly how good you are in the kitchen, regardless of whether someone's watching or what recipe you're following.{{/M}}

**Measurement error** is all the random stuff that messes with the score but doesn't reflect the person's true ability. {{M}}It's like when you're cooking and the smoke alarm goes off, distracting you, or when the oven temperature runs hot that day, or you're exhausted from a bad night's sleep.{{/M}} In testing situations, this includes things like:

- The testing room being too hot or noisy
- The person feeling sick that day
- Ambiguous wording in questions
- Lucky guesses on multiple-choice items
- Simple fatigue from a long test

Classical Test Theory assumes that true scores are stable. They don't change based on which version of a test you take or who scores it. But measurement error? That's unpredictable and random, changing every time.

## Understanding Reliability: What Does "Consistent" Really Mean?

Test reliability tells you how much you can trust that a test gives consistent information. When a test has high reliability, you know that most of what you're seeing in the scores reflects true differences between people, not random error.

Here's the key insight: **A reliability coefficient tells you directly what percentage of score variability comes from true differences versus error.**

Let's say a depression inventory has a reliability coefficient of .85. This means:
- 85% of the differences you see in people's scores reflect actual differences in depression levels
- 15% is just noise from measurement error

{{M}}Imagine you're checking your bank account balance on a banking app. If the app is highly reliable, the number you see reflects your actual balance. But if it's unreliable, sometimes it might show you're $100 short because of a glitch, or $50 over because a transaction didn't update. You want that number to be trustworthy when you're deciding whether you can afford something.{{/M}}

### What Counts as "Good Enough" Reliability?

The acceptable level depends on what's at stake:

| Test Type | Minimum Acceptable | Why |
|-----------|-------------------|-----|
| Research measures, preliminary screening | .70 | Lower stakes; you're looking at group averages or just getting initial information |
| Clinical diagnosis, personnel selection, high-stakes decisions | .90+ | You're making important decisions about someone's life. You need to be very confident |
| Personality and attitude measures | .70-.80 | These traits are inherently more variable and harder to measure precisely |
| Cognitive ability tests | .80-.90+ | These should be highly reliable given their standardized nature |

## Four Ways to Measure Reliability

Just like {{M}}you might evaluate a new restaurant by checking multiple sources (online reviews, asking friends, trying it yourself at different times{{/M}}) there are different ways to evaluate test reliability. Each method answers a different question about consistency.

### 1. Test-Retest Reliability: Consistency Over Time

This method checks whether people get similar scores when they take the same test twice, with some time in between.

**The process:**
- Give the test to a group of people
- Wait days, weeks, or months
- Give them the exact same test again
- Calculate the correlation between the two sets of scores

**When it's useful:** Test-retest reliability matters for characteristics that should be stable over time, like intelligence or enduring personality traits.

**When it's not appropriate:** Don't use this for things that change rapidly, like current mood or state anxiety. {{M}}It's like weighing yourself twice to check if your scale is consistent. That works if you're checking the scale's reliability, but not if you're trying to track actual weight loss over time.{{/M}}

### 2. Alternate Forms Reliability: Consistency Across Different Versions

Some tests have multiple versions (Form A, Form B, etc.) to prevent people from memorizing answers or to allow retesting.

**The process:**
- Give Form A to a group
- Give Form B to the same group (either immediately or later)
- Correlate the scores

**When it's useful:** Essential when your test has multiple forms and you need to know they're interchangeable. Also tells you about stability over time if you administer the forms at different times.

{{M}}Think about taking the driver's license test at different DMV locations. You want to know that passing at one location is just as meaningful as passing at another. That the different versions are equivalent.{{/M}}

### 3. Internal Consistency Reliability: Do the Items Work Together?

This checks whether all the items on your test are measuring the same thing consistently.

**Common methods:**

**Coefficient Alpha (Cronbach's Alpha):** Calculates the average correlation among all items on the test. This is the most widely used method.

**Kuder-Richardson 20 (KR-20):** A version of coefficient alpha specifically for items scored as right/wrong (dichotomous scoring).

**Split-Half Reliability:** Divide the test in half (usually odd-numbered items vs. even-numbered items), correlate the two halves, then apply the Spearman-Brown prophecy formula to estimate what the full test's reliability would be.

**Why the correction?** Because shorter tests are less reliable, split-half gives you the reliability of two half-length tests. The Spearman-Brown formula corrects this to estimate the full test's reliability.

**Important limitation:** Internal consistency is NOT appropriate for speed tests (where you're measuring how fast someone completes tasks). Why? Because on speed tests, people who get far in the test answer more items correctly, making it look like the items are highly consistent when really you're just measuring speed. For speed tests, use test-retest or alternate forms instead.

### 4. Inter-Rater Reliability: Consistency Across Scorers

When tests require judgment to score (like evaluating therapy session quality, rating behavioral observations, or scoring projective tests) you need to know that different raters would give similar scores.

**Methods:**

**Percent Agreement:** Simple calculation: What percentage of the time do two raters agree? Easy to understand but has a major flaw. It doesn't account for chance agreement.

{{M}}Imagine two people randomly guessing "yes" or "no" to coin flips. They'd agree about 50% of the time just by chance, even though neither knows what they're doing.{{/M}}

**Cohen's Kappa:** A more sophisticated measure that corrects for chance agreement. It's used when two raters are assigning ratings on a nominal scale (categories with no inherent order).

**The Danger of Consensual Observer Drift**

Here's something that can quietly ruin your inter-rater reliability: when raters talk to each other while rating, they start agreeing more and more with each other. But not necessarily with reality. Their ratings become consistent but inaccurate.

{{M}}It's like when two friends binge-watch a TV series together and start finishing each other's sentences about what will happen next. They're very consistent with each other, but they might both be completely wrong about the plot.{{/M}}

**How to prevent it:**
- Keep raters working independently
- Provide thorough training before rating begins
- Regularly check ratings against a gold standard

## What Makes Reliability Coefficients Go Up or Down?

Three key factors affect how reliable your test will be:

### 1. Content Homogeneity

Tests measuring one unified thing tend to be more reliable than tests measuring several different things.

A depression inventory asking only about sadness, crying, and hopelessness will typically have higher internal consistency than a general mental health screening that asks about depression, anxiety, and psychosis all mixed together.

### 2. Range of Scores (Sample Heterogeneity)

You get higher reliability coefficients when your sample includes people across the full spectrum of whatever you're measuring. High, medium, and low scorers.

{{M}}Imagine trying to test whether a thermometer is accurate by only measuring temperatures between 70-72 degrees. You'd have trouble seeing if it really works across the full range. But if you measure temperatures from 0 to 100 degrees, you can really see how consistent it is.{{/M}}

If you only test your anxiety measure on people with severe anxiety disorders, you're restricting the range, and your reliability coefficient will be artificially low.

### 3. Guessing

The easier it is to guess the right answer, the lower your reliability will be. Random guessing adds error to scores.

True/false questions are the worst for this. 50% Chance of guessing correctly. Multiple-choice with four options? 25% chance. The more options, the less impact guessing has on reliability.

## From Reliability Coefficient to Reliability Index

Some psychometricians distinguish between these two terms:

- **Reliability coefficient** (what we've been discussing): The proportion of observed score variance that's due to true score variance
- **Reliability index**: The theoretical correlation between observed scores and true scores, calculated as the square root of the reliability coefficient

For example, if a test has a reliability coefficient of .81, the reliability index would be √.81 = .90

You won't use this much in practice, but you might see it on the exam.

## Item Analysis: Building Better Tests

When you're creating a new test, you need to figure out which items are worth keeping. That's where item analysis comes in. You're looking at two key characteristics for each item.

### Item Difficulty (p)

The **p-value** tells you what percentage of test-takers got the item right.

**Calculation:** Number who answered correctly ÷ Total number of test-takers

Example: If 60 out of 100 people answered an item correctly, p = 60/100 = .60

**Interpreting p-values:**
- p = 1.0: Everyone got it right (very easy)
- p = .50: Half got it right (moderate difficulty)
- p = 0: No one got it right (very difficult)

**What's optimal?** For most tests, you want moderately difficult items (p = .30 to .70). Why? Because very easy or very hard items don't help you tell people apart.

**Special cases:**

**Mastery tests:** These are designed to check whether someone has achieved a specific level of competence. {{M}}Think of a licensing exam where you need to verify someone knows at least 80% of critical safety information.{{/M}} For these, you want p-values matching the mastery level (e.g., p = .80 for an 80% mastery test).

**Accounting for guessing:** The optimal difficulty lies halfway between 1.0 and the guessing probability.
- Four-option multiple choice: guessing probability = .25, so optimal p = (1.0 + .25)/2 = .625
- True/false: guessing probability = .50, so optimal p = (1.0 + .50)/2 = .75

### Item Discrimination Index (D)

The **D-value** tells you whether an item successfully distinguishes between high-performing and low-performing test-takers.

**Calculation:**
1. Identify the top 27% of test-takers (based on total test scores)
2. Identify the bottom 27% of test-takers
3. Calculate: D = (% of high scorers who got it right). (% Of low scorers who got it right)

Example: If 85% of high scorers and 40% of low scorers answered correctly:
D = .85. .40 = .45

**Interpreting D-values:**
- D = +1.0: All high scorers got it right, no low scorers did (perfect discrimination)
- D = 0: Equal percentages of high and low scorers got it right (no discrimination)
- D = -1.0: All low scorers got it right, no high scorers did (something's wrong!)

**What's acceptable?** Generally, you want D ≥ .30

**Important connection:** Item difficulty affects discrimination. Moderately difficult items can discriminate better because they give both groups room to show differences. {{M}}It's like trying to judge running ability, if you give everyone a 10-foot race, even slow runners finish quickly, so you can't tell them apart. A 5K race better shows the differences.{{/M}}

## Standard Error of Measurement: Embracing Uncertainty

Here's a truth that makes testing more honest: When a test's reliability is less than perfect (which is always), you can't be certain that someone's obtained score is their true score.

The **standard error of measurement (SEM)** quantifies this uncertainty.

**Formula:** SEM = SD × √(1. Reliability coefficient)

Where SD is the test's standard deviation.

**Example calculation:**
- Test has SD = 15 and reliability = .91
- SEM = 15 × √(1. .91)
- SEM = 15 × √.09
- SEM = 15 × .3
- SEM = 4.5

### Confidence Intervals: Showing the Uncertainty

Rather than reporting a single score, we often report a confidence interval that shows the range where someone's true score likely falls.

**The simple rules:**
- 68% confidence interval: Obtained score ± 1 SEM
- 95% confidence interval: Obtained score ± 2 SEM
- 99% confidence interval: Obtained score ± 3 SEM

**Example:** Someone scores 110 on an IQ test with SEM = 5.

| Confidence Level | Calculation | Range |
|-----------------|-------------|-------|
| 68% | 110 ± (1 × 5) | 105-115 |
| 95% | 110 ± (2 × 5) | 100-120 |
| 99% | 110 ± (3 × 5) | 95-125 |

{{M}}Think about this when you're explaining test results to a client. Instead of saying "Your IQ is 110," you might say "Based on this test, we're 95% confident your IQ falls between 100 and 120." It's more honest and prevents over-interpretation of small score differences.{{/M}}

## Item Response Theory: The Modern Alternative

Item Response Theory (IRT) represents a more sophisticated approach to test development that's increasingly important, especially for computerized testing.

### Key Differences from Classical Test Theory

| Classical Test Theory | Item Response Theory |
|----------------------|---------------------|
| Test-based (focuses on total scores) | Item-based (focuses on individual items) |
| Sample-dependent (item statistics change with different groups) | Sample-invariant (item properties stable across groups) |
| Less suitable for adaptive testing | Ideal for adaptive testing |
| Simpler to understand and calculate | More complex but more powerful |

### The Core Idea: Item Characteristic Curves

IRT examines how each item relates to the underlying trait (the latent trait) you're measuring. Ability, depression severity, extroversion, etc.

For each item, you create an **Item Characteristic Curve (ICC)** that shows:
- **X-axis:** Level of the trait (low to high)
- **Y-axis:** Probability of endorsing or answering correctly (0 to 1.0)

### Three Item Parameters

Depending on which IRT model you use (one-, two-, or three-parameter), the ICC tells you:

**1. Difficulty Parameter (b):** What level of the trait do you need to have a 50% chance of answering correctly?
- Items on the left side of the graph: easier (endorsed by people with lower trait levels)
- Items on the right side: harder (only endorsed by people with higher trait levels)

**2. Discrimination Parameter (a):** How well does this item distinguish between people just above and just below the difficulty level?
- Indicated by the slope of the curve
- Steeper slope = better discrimination
- {{M}}A highly discriminating item is like a precise filter that clearly separates people right at a certain skill level, while a poorly discriminating item is like a cloudy lens that can't quite distinguish between similar ability levels.{{/M}}

**3. Guessing Parameter (c):** What's the probability of getting this item right just by guessing?
- Shown by where the curve crosses the y-axis
- Closer to 0 = harder to guess correctly

### Why IRT Matters for Practice

**Computerized Adaptive Testing:** IRT makes it possible to give each person a customized test that adjusts to their ability level.

{{M}}Instead of everyone taking the same 100-question test, imagine a system that starts with medium-difficulty questions, then adapts: if you get them right, it gives you harder ones; if you get them wrong, it gives you easier ones. You end up with a shorter, more efficient test that's tailored to your level.{{/M}}

This is how the GRE and many modern licensing exams work. It's only possible because IRT lets us precisely calibrate each item's difficulty and discrimination properties.

## Common Misconceptions

**"A reliability of .70 means the test is 70% accurate."**
Not quite. It means that 70% of the variance in scores is due to true differences, and 30% is error. It doesn't directly tell you about accuracy (which relates to validity, not reliability).

**"If a test has high reliability, it must be measuring what it claims to measure."**
Wrong. A test could consistently measure the wrong thing. Reliability is necessary but not sufficient for validity. {{M}}A bathroom scale might consistently give you the same reading every time (reliable), but if it's always 10 pounds off, it's not accurate (not valid).{{/M}}

**"Internal consistency reliability is always the best method to use."**
No. It's inappropriate for speed tests and for tests measuring multiple different constructs. Choose your reliability method based on what the test measures and how it will be used.

**"Small differences in scores are meaningful if the test is reliable."**
Not necessarily. Always consider the standard error of measurement. Two scores need to differ by at least 1-2 SEMs before you can be confident they reflect real differences.

## Practice Tips for Remembering

**The Reliability Coefficient Mnemonic:**
Remember "T/T+E" for the reliability coefficient concept:
- **T**rue score variance on **T**op
- **T**rue score variance plus **E**rror variance on the bottom
- Reliability = T/(T+E)

**The Four Reliability Types:**
Use "TAII" (sounds like "tie"):
- **T**est-retest
- **A**lternate forms
- **I**nternal consistency
- **I**nter-rater

**Confidence Intervals:**
Remember "1-2-3 for 68-95-99":
- 1 SEM = 68%
- 2 SEMs = 95%
- 3 SEMs = 99%

**Item Difficulty:**
"**p** stands for **p**ercent who **p**assed" (got it right)

**Item Discrimination:**
"**D** = **D**ifference" (between high and low scorers)

## Key Takeaways

- **Classical Test Theory** says obtained scores = true scores + error; reliability tells you what proportion is true score variance

- **Reliability coefficients** range from 0 to 1.0 and are interpreted directly as the percentage of variance due to true differences (not error)

- **Four reliability methods:** test-retest (consistency over time), alternate forms (consistency across versions), internal consistency (items measuring the same thing), and inter-rater (consistency across scorers)

- **Internal consistency is inappropriate for speed tests**. Use test-retest or alternate forms instead

- **Higher reliability comes from:** homogeneous content, unrestricted score range, and less susceptibility to guessing

- **Standard error of measurement** quantifies score uncertainty; use it to create confidence intervals (±1 SEM = 68%, ±2 SEM = 95%, ±3 SEM = 99%)

- **Item difficulty (p)** = proportion who answered correctly; optimal range is typically .30-.70

- **Item discrimination (D)** = difference between top 27% and bottom 27%; want D ≥ .30

- **Item Response Theory** is item-based (not test-based), produces sample-invariant parameters, and enables computerized adaptive testing

- **IRT uses Item Characteristic Curves** to show difficulty, discrimination, and guessing parameters for each item

Understanding these concepts will help you select appropriate tests, interpret scores responsibly, and explain results clearly to clients and colleagues. When you see a test manual in practice, you'll know exactly which reliability information to look for and how to interpret what you find.