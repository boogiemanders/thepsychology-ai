{
  "questions": [
    {
      "stem": "In classical test theory, an observed test score (X) is conceptualized as the sum of which two components?",
      "options": [
        "True score variability and measurement error",
        "Observed score and reliability coefficient",
        "Item difficulty and item discrimination",
        "Standard error and confidence interval"
      ],
      "answer": "True score variability and measurement error",
      "explanation": "Classical test theory posits that an obtained score (X) equals the true score (T) plus random error (E), representing measurement error.",
      "kn": "KN29",
      "kn_explanation": "Understanding the basics of test score components and psychometric theory directly relates to psychometric theories and measurement (KN29).",
      "difficulty": "easy",
      "quality_comment": "This foundational question clearly assesses core understanding of classical test theory; distractors are plausible but distinct."
    },
    {
      "stem": "Which reliability method is most appropriate for a test that measures a stable trait and requires consistency of scores over time?",
      "options": [
        "Test-retest reliability",
        "Alternate forms reliability",
        "Internal consistency reliability",
        "Inter-rater reliability"
      ],
      "answer": "Test-retest reliability",
      "explanation": "Test-retest reliability assesses score stability over time, making it suitable for tests measuring stable characteristics.",
      "kn": "KN29",
      "kn_explanation": "This question assesses knowledge of reliability assessment methods critical to test construction and evaluation under psychometric theories (KN29).",
      "difficulty": "medium",
      "quality_comment": "The question is clear with well-differentiated options, focusing on appropriate reliability methods."
    },
    {
      "stem": "Cronbach’s alpha is commonly used to estimate which type of reliability?",
      "options": [
        "Internal consistency reliability",
        "Test-retest reliability",
        "Alternate forms reliability",
        "Inter-rater reliability"
      ],
      "answer": "Internal consistency reliability",
      "explanation": "Cronbach’s alpha calculates the average inter-item correlation to estimate the degree to which items measure the same construct within a test.",
      "kn": "KN29",
      "kn_explanation": "This question covers knowledge of a major internal consistency reliability statistic, important in evaluating psychometric properties (KN29).",
      "difficulty": "medium",
      "quality_comment": "Options are plausible, and the question targets a frequently tested reliability concept."
    },
    {
      "stem": "Which of the following factors tends to increase the reliability coefficient of a test?",
      "options": [
        "Homogeneity of test content",
        "Restriction of test score range",
        "Increased guessing probability on test items",
        "Use of short test forms"
      ],
      "answer": "Homogeneity of test content",
      "explanation": "Tests with homogeneous content often have higher internal consistency, thereby increasing reliability coefficients.",
      "kn": "KN29",
      "kn_explanation": "The question addresses factors affecting reliability magnitude, essential in psychometric test evaluation (KN29).",
      "difficulty": "medium",
      "quality_comment": "Distractors address common misconceptions, enhancing discriminability for test-takers."
    },
    {
      "stem": "An item on a multiple-choice test has a difficulty index (p) of 0.30. Which statement best characterizes this item?",
      "options": [
        "It is a relatively difficult item, with only 30% of examinees answering correctly",
        "It is a relatively easy item, with 70% of examinees answering correctly",
        "It discriminates poorly between high and low scorers",
        "It is too easy and should be removed from the test"
      ],
      "answer": "It is a relatively difficult item, with only 30% of examinees answering correctly",
      "explanation": "The difficulty index (p) represents the proportion of examinees answering correctly, so p = 0.30 means 30% answered correctly, indicating difficulty.",
      "kn": "KN29",
      "kn_explanation": "This question evaluates understanding of item difficulty, a critical item analysis concept in test construction (KN29).",
      "difficulty": "easy",
      "quality_comment": "The question is straightforward with clear numeric interpretation; distractors are reasonable but incorrect."
    },
    {
      "stem": "Which method corrects for the underestimation of reliability that occurs when splitting a test in half for the calculation of split-half reliability?",
      "options": [
        "Spearman-Brown prophecy formula",
        "Kuder-Richardson 20 formula",
        "Cronbach’s alpha",
        "Cohen’s kappa coefficient"
      ],
      "answer": "Spearman-Brown prophecy formula",
      "explanation": "The Spearman-Brown formula adjusts the split-half reliability coefficient to estimate the reliability of the full-length test.",
      "kn": "KN29",
      "kn_explanation": "This question relates to methods for estimating reliability coefficients and their correction, grounded in psychometric theory (KN29).",
      "difficulty": "hard",
      "quality_comment": "The question is technically detailed and challenges test-takers’ knowledge of specific reliability corrections."
    },
    {
      "stem": "A test has a reliability coefficient of .81. What is the reliability index (index of reliability), as defined by taking the square root of the coefficient?",
      "options": [
        "0.90",
        "0.81",
        "0.18",
        "1.62"
      ],
      "answer": "0.90",
      "explanation": "The reliability index equals the square root of the reliability coefficient: √0.81 = 0.90, representing the correlation between observed and true scores.",
      "kn": "KN29",
      "kn_explanation": "Understanding the relationship between reliability coefficients and their indices is part of psychometric measurement concepts (KN29).",
      "difficulty": "medium",
      "quality_comment": "Numerical but basic calculation question, with distractors that test misunderstandings of the formula."
    },
    {
      "stem": "Inter-rater reliability is most accurately estimated by which statistic that accounts for chance agreement when two raters assign nominal ratings?",
      "options": [
        "Cohen’s kappa coefficient",
        "Percent agreement",
        "Cronbach’s alpha",
        "Spearman-Brown prophecy formula"
      ],
      "answer": "Cohen’s kappa coefficient",
      "explanation": "Cohen’s kappa adjusts for chance agreement between two raters when ratings are categorical, making it more accurate than percent agreement.",
      "kn": "KN29",
      "kn_explanation": "Inter-rater reliability measurement techniques are integral to psychometric evaluation in assessment (KN29).",
      "difficulty": "medium",
      "quality_comment": "Good clarity and plausible distractors, focusing the test-taker on differences between reliability coefficients."
    },
    {
      "stem": "In item response theory (IRT), the difficulty parameter of an item represents:",
      "options": [
        "The level of the latent trait where there is a 50% chance of endorsing or answering correctly",
        "The average score of examinees on the item",
        "The slope of the item characteristic curve indicating discrimination",
        "The chance of correctly guessing the answer"
      ],
      "answer": "The level of the latent trait where there is a 50% chance of endorsing or answering correctly",
      "explanation": "In IRT, the difficulty (location) parameter indicates the latent trait level associated with a 50% probability of a correct or endorsed response.",
      "kn": "KN29",
      "kn_explanation": "This assesses knowledge of item parameters central to modern test theory and item response theory used in test construction (KN29).",
      "difficulty": "hard",
      "quality_comment": "The question probes advanced understanding of IRT model parameters with balanced plausible options."
    },
    {
      "stem": "What is the standard error of measurement (SEM) if a test has a standard deviation of 10 and a reliability coefficient of 0.84?",
      "options": [
        "4.0",
        "8.4",
        "1.6",
        "2.0"
      ],
      "answer": "4.0",
      "explanation": "SEM = SD × √(1 - reliability) = 10 × √(1 - 0.84) = 10 × √0.16 = 10 × 0.4 = 4.0.",
      "kn": "KN29",
      "kn_explanation": "Calculating SEM requires understanding of reliability and variability concepts in psychological measurement (KN29).",
      "difficulty": "medium",
      "quality_comment": "Math-focused question with clear, calculable options, typical of reliability statistics in psychometrics."
    },
    {
      "stem": "An obtained test score is 85. The test's standard error of measurement is 5. What is the 95% confidence interval for the examinee's true score?",
      "options": [
        "75 to 95",
        "80 to 90",
        "70 to 100",
        "85 to 95"
      ],
      "answer": "75 to 95",
      "explanation": "A 95% confidence interval is obtained by adding and subtracting 2 SEMs: 85 ± 2(5) = 85 ± 10 = 75 to 95.",
      "kn": "KN29",
      "kn_explanation": "This question tests applying SEM to construct confidence intervals, a key psychometric interpretation skill (KN29).",
      "difficulty": "easy",
      "quality_comment": "Clear calculation with properly distributed plausible distractors reflecting common mistakes."
    },
    {
      "stem": "Consensual observer drift is most likely to affect the reliability of which type of assessment?",
      "options": [
        "Subjectively scored ratings by multiple raters",
        "Objective multiple-choice cognitive tests",
        "Self-report personality inventories",
        "Physiological measures"
      ],
      "answer": "Subjectively scored ratings by multiple raters",
      "explanation": "Consensual observer drift occurs when raters influence one another’s ratings, increasing agreement but potentially reducing accuracy in subjective assessments.",
      "kn": "KN29",
      "kn_explanation": "Understanding factors affecting inter-rater reliability for subjective measures is important in psychometric theory and assessment (KN29).",
      "difficulty": "medium",
      "quality_comment": "Well-constructed question targeting nuances of rater-related reliability error, with plausible alternatives."
    },
    {
      "stem": "Which item discrimination index (D) value is generally considered acceptable for test items?",
      "options": [
        "0.30 or higher",
        "0.10 or lower",
        "-0.25 or below",
        "Exactly zero"
      ],
      "answer": "0.30 or higher",
      "explanation": "A discrimination index of 0.30 or greater indicates good ability of an item to differentiate between high- and low-scoring examinees.",
      "kn": "KN29",
      "kn_explanation": "This question focuses on item analysis and interpretation of discrimination values, central to psychometric test development (KN29).",
      "difficulty": "easy",
      "quality_comment": "Clear and focused question; distractors reflect common misunderstanding about discrimination indices."
    },
    {
      "stem": "Alternate forms reliability is best described as an assessment of:",
      "options": [
        "The consistency of scores across two different versions of a test",
        "The consistency of scores over repeated test administrations",
        "The internal consistency of items within the same test",
        "The agreement between two raters’ scores"
      ],
      "answer": "The consistency of scores across two different versions of a test",
      "explanation": "Alternate forms reliability measures the correlation between scores on different but equivalent versions of the same test.",
      "kn": "KN29",
      "kn_explanation": "This question asks about a key method for estimating reliability, covered in psychometric test evaluation (KN29).",
      "difficulty": "easy",
      "quality_comment": "Options are concise and clearly different; the question is well-targeted to a core reliability concept."
    },
    {
      "stem": "Item response theory (IRT) differs from classical test theory (CTT) primarily in that IRT:",
      "options": [
        "Focuses on examinee responses to individual test items rather than total test scores",
        "Assumes test scores are composed of true score plus error score",
        "Uses total test scores to estimate item difficulty and discrimination",
        "Does not require large sample sizes for parameter estimation"
      ],
      "answer": "Focuses on examinee responses to individual test items rather than total test scores",
      "explanation": "IRT focuses on modeling the probability of specific item responses based on latent traits, differing from CTT's emphasis on total test scores.",
      "kn": "KN30",
      "kn_explanation": "This item assesses understanding of differing assessment theories and models, specifically the contrast between CTT and IRT (KN30).",
      "difficulty": "medium",
      "quality_comment": "The question clearly distinguishes key features of IRT versus traditional measurement models with plausible options."
    },
    {
      "stem": "Which of the following best characterizes the use of item characteristic curves (ICCs) in item response theory?",
      "options": [
        "They plot the probability of endorsing or correctly answering an item against levels of the latent trait",
        "They illustrate the variability of total test scores across examinees",
        "They show the correlation between test items and overall test scores",
        "They represent the internal consistency of test items"
      ],
      "answer": "They plot the probability of endorsing or correctly answering an item against levels of the latent trait",
      "explanation": "ICCs graphically display how the probability of a correct or endorsed response varies with latent trait level in IRT models.",
      "kn": "KN30",
      "kn_explanation": "This question targets the interpretation of key graphical tools used in modern test theories aligning with assessment models (KN30).",
      "difficulty": "medium",
      "quality_comment": "Well-constructed with focused content, this question taps understanding of foundational IRT concepts."
    },
    {
      "stem": "Which parameter of an item response theory (IRT) model indicates how well an item differentiates among examinees with different levels of a latent trait?",
      "options": [
        "Discrimination parameter",
        "Difficulty parameter",
        "Guessing parameter",
        "Reliability coefficient"
      ],
      "answer": "Discrimination parameter",
      "explanation": "The discrimination parameter reflects how sharply an item can distinguish between examinees with low versus high levels of the trait.",
      "kn": "KN30",
      "kn_explanation": "This item highlights understanding of specific IRT parameters central to assessment theories and models (KN30).",
      "difficulty": "medium",
      "quality_comment": "This question clarifies key distinctions among item parameters, reinforcing core conceptual knowledge required for the EPPP."
    }
  ]
}