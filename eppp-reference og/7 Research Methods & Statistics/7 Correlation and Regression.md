# Correlation and Regression


Correlation is used to determine the degree of association between two or more variables. It’s often of interest when the goal is to use one or more predictors to estimate status on one or more criteria. (In the context of correlation, an independent variable is usually referred to as the predictor or X variable, and a dependent variable is referred to as the criterion or Y variable.) 

Most correlation coefficients range from -1.0 (a perfect negative correlation) to +1.0 (a perfect positive correlation), and many coefficients are symbolized with the letter “r.” It’s important to notice the subscript of a correlation coefficient: When it contains two different letters or numbers (e.g., “xy”), this means the coefficient is a measure of the relationship between two different variables. Depending on the context, this coefficient can just be a measure of the relationship between variables or it can be a criterion-related validity coefficient or a factor loading. In contrast, when the subscript contains two of the same letters or numbers (e.g., “xx”), it’s a reliability coefficient. This summary addresses correlation coefficients for two different variables; reliability coefficients are covered in a test construction summary.  

**Assumptions:** The use of most correlation coefficients is based on three assumptions. The first assumption is that the relationship between variables is linear. When the relationship is nonlinear, the correlation coefficient may underestimate their actual relationship. The second assumption is that there’s an unrestricted range of scores for all variables. When there’s a restricted range (e.g., when the sample included only people with average scores rather than low, average, and high scores), the correlation coefficient may underestimate the actual relationship. And the third assumption is that there’s homoscedasticity – i.e., that the variability of criterion scores is similar for all predictor scores. When this assumption is violated (when the variability of criterion scores differs for different predictor scores), the use of a regression equation to predict people’s criterion scores from their obtained predictor scores will not have the same accuracy of prediction for all predictor scores.

**Bivariate Correlation Coefficients:** The choice of a bivariate correlation coefficient is based primarily on the scale of measurement used to measure the two variables.   
(a) The *Pearson r* is also known as the Pearson product moment correlation coefficient and is used when both variables are measured on a continuous (interval or ratio) scale and the relationship between the variables is linear. When the relationship between variables is nonlinear, the Pearson r will underestimate the degree of that relationship. An alternative to the Pearson r is *eta*, which can be used when both variables are continuous and their relationship is linear or nonlinear.
(b) The *Spearman rho* is also known as the Spearman rank correlation coefficient and is used when data on both variables are reported as ranks. 
(c) The *point biserial correlation coefficient* is used when one variable is continuous and the other is a true dichotomy. (A dichotomy is a nominal variable with only two categories.) The distinction between being pregnant or not being pregnant is a true dichotomy.
(d) The *biserial correlation coefficient* is used when one variable is continuous and the other is an artificial dichotomy. An artificial dichotomy occurs when a continuous variable is dichotomized. Final exam scores represent an artificial dichotomy when a cutoff score is used to divide the scores into two categories – pass and fail.
(e) The *contingency correlation coefficient* is used when both variables are measured on a nominal scale.

**Coefficient of Determination:** The bivariate correlation coefficient can be interpreted directly as the degree of association between the predictor and criterion. Alternatively, it can be squared to derive a *coefficient of determination*, which is a measure of shared variability and indicates the amount of variability in one variable that’s explained (accounted for) by variability in the other variable. For instance, assume that the predictor is a measure of job knowledge and the criterion is a measure of job performance and that their correlation coefficient is .70: .70 squared is .49. This means that 49% of variability in job performance is explained by differences in level of job knowledge, while the remaining 51% is due to other factors such as job experience, job motivation, and work conditions.  

**Regression Analysis:** Correlation is often of interest because the goal is to use obtained predictor scores to estimate criterion scores. Prediction is made possible with *regression analysis*, which uses the data collected in a correlational study to produce a regression equation. This equation is then used to predict a person’s criterion score from his/her obtained predictor score. The accuracy of prediction increases as the correlation between the predictor and criterion increases.   

**Multivariate Correlational Techniques:**  Multivariate correlational techniques are extensions of bivariate correlation and regression analysis. They make it possible to use two or more predictors to estimate status on one or more criteria.   

(a) *Multiple regression* is the appropriate technique when two or more predictors will be used to estimate status on a single criterion that’s measured on a continuous scale. There are two forms of multiple regression: Simultaneous (standard) multiple regression involves entering data on all predictors into the equation simultaneously. Stepwise multiple regression involves adding or subtracting one predictor at a time to the equation in order to identify the fewest number of predictors that are needed to make accurate predictions. When using multiple regression, the optimal circumstance is for each predictor to have a high correlation with the criterion but low correlations with other predictors since this means that each predictor is providing unique information. When predictors are highly correlated with one another, this is referred to as multicollinearity.

(b) *Canonical correlation* is the appropriate technique when two or more continuous predictors will be used to estimate status on two or more continuous criteria.

(c) *Discriminant function analysis* is the appropriate technique when two or more predictors will be used to estimate status on a single criterion that’s measured on a nominal scale. *Logistic regression* is the alternative to discriminant function analysis when the assumptions for discriminant function analysis are not met (e.g., when scores on the predictors are not normally distributed). 

**Structural Equation Modeling (SEM):** SEM includes several multivariate statistical techniques that are used to test hypotheses about relationships among observed and latent variables and can be viewed as a combination of factor analysis and multiple regression (Ullman, 2001). *Observed variables* are also known as manifest variables and indicators and are directly observed and measured; *latent variables* are also known as factors and constructs and cannot be directly observed or measured but, instead, are inferred from observed variables. Observed and latent variables can be exogeneous or endogenous: *Exogeneous variables* are not explained or predicted by any other variable included in the structural equation model; *endogenous variables* are explained or predicted by other variables in the model. As an example, Li and Wang (2021) used SEM to test the hypothesis that the relationship between structural social support and loneliness is mediated by functional social support. In their model, structural social support, functional social support, and loneliness were latent variables that were each measured by several observed variables. Loneliness, for instance, was measured with three items from the UCLA Loneliness Scale, with each item being treated as a separate observed variable. In this study, the three latent variables were endogenous and the observed variables measuring the latent variables were exogeneous. (Note that some authors describe path analysis, which is used to test relationships among observed variables only, as a precursor of SEM while others consider it to be an atypical type of SEM.)

SEM consists of five basic steps: The first step is model specification, which involves specifying a model that describes the hypothesized relationships among the observed and latent variables based on theory and/or previous empirical findings. These relationships are depicted in a path diagram, in which observed variables are indicated with squares or rectangles, latent variables are indicated with circles or ellipses, and relationship between any two variables is indicated with an arrow. The second and third steps – model identification and model estimation – are complex, and it is not necessary to be familiar with their procedures for the exam. The fourth step is model evaluation which involves using the goodness-of-fit index (GFI) or other method to determine the model’s fit with the obtained data. The fifth step is model modification which involves revising the model, if necessary, to better fit the data.

