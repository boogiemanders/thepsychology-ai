# Inferential Statistical Tests


**Parametric versus Nonparametric Statistical Tests:**  Inferential statistical tests are divided into two types – nonparametric and parametric. *Nonparametric tests* are used to analyze nominal and ordinal data and include the chi-square test, while *parametric tests* are used to analyze interval and ratio data and include the t-test and analysis of variance. In addition, the use of a parametric test assumes that certain assumptions are met – e.g., that the data are normally distributed and that there is homogeneity of variances, which means that the variances for the different groups are similar. Consequently, even when the dependent variable is measured on an interval or ratio scale, a nonparametric test is ordinarily used when these assumptions are violated and the group sizes are small and unequal.

**Choosing an Inferential Statistical Test:**  The selection of an inferential statistical test requires consideration of several factors. The first factor is the scale of measurement of the data that will be analyzed with the test. The scale of measurement narrows the choices, but it’s sometimes necessary to consider other factors such as the number of independent variables, the number of levels of the independent variable, whether the groups are related or unrelated, if an extraneous variable needs to be controlled, and the number of dependent variables.

**Chi-Square Test:**  The chi-square test is used when the data to be analyzed are nominal data. There are two chi-square tests – the *single-sample chi-square test*, which is also known as the chi-square goodness-of-fit test, and the *multiple-sample chi-square test*, which is also known as the chi-square test for contingency tables. It will be easier to determine which chi-square test to use for an exam question if you substitute the word “variable” for “sample”: The single-sample (single-variable) chi-square test is used to analyze data from a descriptive study that includes only one variable, while the multiple-sample (multiple-variable) chi-square test is used to analyze data from (a) a descriptive study that has two or more variables that can’t be identified as independent or dependent variables or (b) an experimental study that has independent and dependent variables. Remember that, when determining the number of variables for the chi-square test, you count <u>all</u> of the variables. 

As an example, you would use the single-sample chi square test to analyze data collected in a study to determine whether college undergraduates prefer to use a hard-copy textbook or an online textbook for their introductory statistics class. This is a descriptive study with a single nominal variable that has two categories, and the single-sample chi-square test would be used to compare the number of students in the two categories. If this study is expanded to include type of course (face-to-face course or online course), the study is still a descriptive study but it includes two variables, and a statistical test will be used to compare the number of students in the four categories (prefer hard-copy text/face-to-face course, prefer hard-copy text/online course, prefer online text/face-to-face course, and prefer online-text/online course). Because the study includes two variables and the data to be analyzed are nominal (the number of subjects in each nominal category), the multiple-sample chi-square test is the appropriate statistical test.

**Student’s t-Test:** The Student’s t-test is used when a study includes one independent variable that has two levels and one dependent variable that’s measured on an interval or ratio scale. In this situation, the t-test will be used to compare two means. For example, the t-test would be used to compare the mean mock EPPP exam scores obtained by psychologists who participated in either a live exam review workshop or an online exam review workshop.
There are three t-tests and the appropriate one depends on how the two means were obtained: The *t-test for a single sample* is used to compare an obtained sample mean to a known population mean. (In this situation, the population is acting as the no-treatment control group.) The *t-test for unrelated samples* is also known as the t-test for uncorrelated samples and is used to compare the means obtained by two groups when subjects in the groups are unrelated – e.g., when subjects were randomly assigned to one of the two groups. Finally, the *t-test for related samples* is also known as the t-test for correlated samples and is used to compare two means when there’s a relationship between subjects in the two groups. This occurs when (a) participants are “natural” pairs (e.g., twins), and members of each pair are assigned to different groups; (b) participants are matched in pairs on the basis of their pretest scores or status on an extraneous variable, and members of each pair are assigned to different groups; or (c) a single-group pretest-posttest design is used and subjects are “paired” with themselves.

**One-Way Analysis of Variance:** The one-way analysis of variance (ANOVA) is the appropriate statistical test when a study includes one independent variable that has more than two levels and one dependent variable that’s measured on an interval or ratio scale and the groups are unrelated. It would be the appropriate statistical test to compare the effects of cognitive-behavior therapy, interpersonal therapy, and acceptance and commitment therapy on severity of depressive symptoms when clinic clients are randomly assigned to one of the therapies and symptoms are measured on an interval or ratio scale.
Although the one-way ANOVA can be used when a study has one independent variable with only two levels, the t-test has traditionally been used in this situation. Also, separate t-tests can be used to compare three or more levels of a single independent variable, but this would require conducting separate t-tests for each pair of means. A disadvantage of this approach is that it increases the probability of making a Type I error – i.e., it increases the *experimentwise error rate*. As an example, when the independent variable has three levels and separate t-tests are used to compare means, three t-tests would have to be conducted (Group #1 vs. Group #2, Group #1 vs. Group #3, and Group #2 vs. Group #3). If alpha is set at .05 for each t-test, this would result in an experimentwise error rate of about .15. When using the one-way ANOVA, all possible comparisons between means are made in a way that maintains the experimentwise error rate at the alpha level set by the researcher. (Note that the terms experimentwise error rate and familywise error rate are often used interchangeably, but that some authors distinguish between the two, with experimentwise error rate referring to the Type I error rate for all statistical analyses made in a research study and familywise error rate referring to the Type I error rate for a subgroup of statistical analyses. For example, in a study with two or more independent variables, analyses of the main effects of each independent variable would be one family and analyses of the interaction effects of the independent variables would be another family.)
The one-way ANOVA produces an *F-ratio*. For the EPPP, you want to know that the numerator of the F-ratio is referred to as the “mean square between” (MSB) and is a measure of variability in dependent variable scores that’s due to treatment effects plus error and that the denominator is referred to as the “mean square within” (MSW) and is a measure of variability that’s due to error only. Whenever the F-ratio is larger than 1.0, this suggests that the independent variable has had an effect on the dependent variable. Whether or not this effect is statistically significant depends on the size of alpha and the degrees of freedom. Calculation of degrees of freedom varies for each type of statistical test. For an F-ratio derived from a one-way ANOVA, two degrees of freedom are calculated, one for MSB and one for MSW: Degrees of freedom for MSB are C-1 and degrees of freedom for MSW are N-C, where C is the number of levels of the independent variable and N is the total number of subjects included in the study. As an example, assume that a study is conducted to compare the effects of three treatments on depressive symptoms and 36 clinic clients are assigned to one of the three treatments. In this situation, the degrees of freedom for MSB are 2 (C-1 = 3-1 = 2), and the degrees of freedom for MSW are 33 (N-C = 36-3 = 33).

**Other Forms of the Analysis of Variance:** The factorial ANOVA, mixed ANOVA, randomized block ANOVA, ANCOVA, MANOVA, and trend analysis are other forms of the analysis of variance that you want to be familiar with for the exam.

(a) The *factorial ANOVA* is an extension of the one-way ANOVA that’s used when a study includes more than one independent variable. It’s also referred to as a two-way ANOVA when the study includes two independent variables, a three-way ANOVA when a study includes three independent variables, etc. A factorial ANOVA produces separate F-ratios for the main effects of each independent variable and their interaction effects.
(b) The *mixed ANOVA* is also known as the split-plot ANOVA and is used when the data were obtained from a study that used a mixed design – i.e., when the study included at least one between-subjects independent variable and at least one within-subjects independent variable.
(c) The *randomized block ANOVA* is used to control the effects of an extraneous variable on a dependent variable by including it as an independent variable and determining its main and interaction effects on the dependent variable. When using the randomized block ANOVA, the extraneous variable is referred as the “blocking variable.”
(d) The *analysis of covariance* (ANCOVA) is also used to control the effects of an extraneous variable on a dependent variable but does so by statistically removing its effects from the dependent variable. When using the ANCOVA, the extraneous variable is the “covariate.”
(e) The *multivariate analysis of variance* (MANOVA) is the appropriate statistical test when a study includes one or more independent variables and two or more dependent variables that are each measured on an interval or ratio scale.
(f) *Trend analysis* is used when a study includes one or more quantitative independent variables and the researcher wants to determine if there’s a significant linear or nonlinear (quartic, cubic, or quadratic) relationship between the independent and dependent variables.
**Planned Comparisons and Post Hoc Tests:** When an analysis of variance produces a statistically significant F-ratio, this indicates that at least one group is significantly different from another group but does not indicate which groups differ significantly from each other. Conducting planned comparisons and post hoc tests are two ways to obtain this information. *Planned comparisons* are also known as planned contrasts and a priori tests. These comparisons are designated before the data is collected and are based on theory, previous research, or the researcher’s hypotheses. For example, assume that a psychology professor at a large university conducts a study to test the hypothesis that adding instructor-led study sessions to her introductory psychology lectures will improve the final exam scores of undergraduate students. To test this hypothesis, she designs a study to evaluate four teaching methods: two that are currently available to students and two new methods that include instructor-led study sessions. The four teaching methods are lectures only (L), lectures with peer-led study sessions (LP), lectures with instructor-led in-person study sessions (LIP), and lectures with instructor-led Zoom study sessions (LIZ). Because the professor is interested only in comparing lectures to lectures with instructor-led study sessions, she will not conduct a one-way analysis of variance but, instead, will use two t-tests to compare the mean final exam scores obtained by students in the L and LIP groups and the mean final exam scores obtained by students in the L and LIZ groups. 
*Post hoc tests* are also known as a posteriori tests and are conducted when an ANOVA produces a significant F ratio. For the teaching method study, if the psychology professor decides she is interested in comparing the effects of all of the teaching methods, she will first conduct a one-way ANOVA. If the ANOVA yields a significant F-ratio, this indicates that at least one teaching method differs significantly from another teaching method but does not indicate which teaching methods differ significantly from each other. Therefore, the professor will use t-tests to compare all possible pairs of group means: L versus LP, L versus LIP, L versus LIZ, LP versus LIP, LP versus LIZ, and LIP versus LIZ.
As noted in the description of the one-way ANOVA, the greater the number of statistical tests used to analyze the data collected in a research study, the greater the *experimentwise error rate*. Consequently, when conducting planned comparisons or post hoc tests, it is desirable to control the experimentwise error rate. One way to do so for both planned comparisons and post hoc tests is to use the Bonferroni procedure, which simply involves dividing alpha by the total number of statistical tests to obtain an alpha level for each test. For example, there are two planned comparisons for the teaching method study and, if the professor sets alpha at .05, alpha would be .025 (.05/2) for each comparison. An alternative for post hoc tests is to use one of the modifications of the t-test that are each appropriate for a different situation and differ in terms of the ways they control the experimentwise error rate. Frequently used post hoc tests include Tukey’s honestly significant difference (HSD) test, the Scheffe test, and the Newman-Keuls test.

**Practical Significance:** The results of a statistical test indicate whether or not the results of a study are statistically significant; however, researchers often want to know if the results have practical significance, which refers to the magnitude of the effects of an intervention – i.e., the intervention’s “effect size.” Cohen’s *d* is one of the methods used to measure effect size and indicates the difference between two groups (a treatment group and a control group or two different treatment groups) in terms of standard deviation units. It’s calculated by dividing the mean difference between the groups on the dependent variable by the pooled standard deviation for the two groups. As an example, if *d* is .50 for treatment and control groups, this means that the treatment group’s mean on the dependent variable was one-half standard deviation above the control group’s mean. Cohen (1969) provided guidelines for interpreting *d*: A *d* less than .2 indicates a small effect of the independent variable, a *d* between .2 and .8 indicates a medium effect, and a *d* larger than .8 indicates a large effect. (Cohen’s *f* is the alternative to Cohen’s *d* when the comparison involves more than two groups.)

**Clinical Significance:** Statistical significance and practical significance do not indicate if the effects of an intervention have clinical significance, which refers to the importance or meaningfulness of the effects. For example, even when an intervention has statistical and practical significance, this does not indicate if the intervention is likely to move an individual from a dysfunctional to a normal level of functioning  (Atkins, Bedics, McGlinchey, & Beauchaine, 2005). The *Jacobson-Truax method* (Jacobson, Follette, & Revenstorf, 1984) is one method for evaluating the clinical significance of an intervention for each participant in a clinical trial or other research study. It involves two steps: The first step is to calculate a reliable change index (RCI) to determine if the difference in an individual’s pretreatment and posttreatment test scores is statistically reliable – i.e., if the difference is due to actual change rather than measurement error. It’s calculated by subtracting the individual’s pretest score from his or her posttest score and dividing the result by the standard error of the difference. The RCI can be positive or negative, depending on whether a high or low test score is indicative of improvement. When the change in scores is in the desired direction and RCI is larger than +1.96 or -1.96, the change is considered reliable (not due to measurement error). The second step is to identify the test cutoff score that distinguishes between dysfunctional and functional behavior or performance to determine if an individual’s posttest score is within the functional range. One way to determine the cutoff score is to calculate the score that is midway between the mean score for the dysfunctional (patient) population and the mean score for the functional (non-patient) population. Finally, using the information derived from these two steps, the individual is classified as recovered (passed RCI and cutoff criteria), improved (passed RCI but not cutoff criteria), unchanged/indeterminate (passed neither criteria), or deteriorated (passed RCI in the unintended direction).
