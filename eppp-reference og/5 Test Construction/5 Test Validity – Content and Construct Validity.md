# Test Validity – Content and Construct Validity


When a test has adequate reliability, this means that test scores can be expected to be consistent, but it does not indicate that the test measures what it was designed to measure. Therefore, after determining that a test has adequate reliability, the next step is to evaluate its validity.

Validity has been traditionally defined as the degree to which a test accurately measures what it was designed to measure, and a distinction has been made between three types of validity – content, construct, and criterion-related. The *Standards for Educational and Psychological Testing* (AERA, APA, & NCME, 2014), however, identifies validity as a unitary concept and defines it as “the degree to which evidence and theory support the interpretation of test scores for proposed uses of tests” (p. 11). It also distinguishes between five sources of validity evidence: evidence based on test content, the response process, the internal structure of the test, relationships with other variables, and the consequences of testing. The three types of validity are incorporated into these sources of evidence and, for the EPPP, you want to be familiar with the three types. Content and construct validity are described in this summary; criterion-related validity is described in a separate summary.

**Content Validity:** Evidence of content validity is important for tests that have been designed to measure one or more content or behavior domains. For example, achievement tests and work samples should have adequate content validity. Content validity is established during the development of a test by clearly defining the domain to be assessed and including items that are a representative sample of that domain. Test items are then systematically reviewed by subject matter experts to ensure the items address all important aspects of the domain.

Content validity is sometimes confused with *face validity* which refers to the extent to which test items “look valid” to examinees. It’s not an actual type of validity but is important whenever a lack of face validity might reduce examinees’ willingness to do their best when answering test items. For some tests (e.g., tests of honesty or criminality), however, face validity is undesirable because it may cause examinees to respond to items in an inaccurate way.
**Construct Validity:** Evidence of construct validity is important for tests that have been designed to measure a hypothetical trait (e.g., intelligence, motivation, introversion) that cannot be directly observed but is inferred from an examinee’s behavior. Establishing a test’s construct validity involves using several procedures including obtaining evidence of the test's convergent and divergent validity: *Convergent validit*y refers to the degree to which scores on the test have high correlations with scores on other measures designed to assess the same or related constructs. *Divergent validity* is also known as discriminant validity and refers to the degree to which scores on the test have low correlations with scores on measures of unrelated constructs.
1. **Multitrait-Multimethod Matrix:** The multitrait-multimethod matrix is a table of correlation coefficients that provide information about a test’s reliability and convergent and divergent validity. When using the matrix, the test to be validated is administered to a sample of examinees along with at least three other measures: a test of the same trait using a different method, a test of an unrelated trait using the same method, and a test of an unrelated trait using a different method. As an example, assume that you want to use the multitrait-multimethod matrix to assess the convergent and divergent validity of a newly developed self-report sociability test for middle-school students, and you know that the research has found that sociability is not correlated with impulsivity. Consequently, you administer the self-report sociability test to a sample of students along with tests of sociability and impulsivity that have already been validated: a teacher report sociability test, a self-report impulsivity test, and a teacher report impulsivity test. You then correlate all pairs of test scores and list the correlation coefficients in the multitrait-multimethod matrix. Four of these coefficients are relevant to the test you are validating:

(a) *Monotrait-Monomethod Coefficient:* The monotrait-monomethod (same trait, same method) coefficient is a reliability coefficient (e.g., coefficient alpha) for the self-report sociability test.
(b) *Monotrait-Heteromethod Coefficient:* The monotrait-heteromethod (same trait, different method) coefficient is the correlation coefficient for the self-report sociability test and the teacher report sociability test. When this coefficient is large, it provides evidence of the self-report sociability test’s convergent validity.
(c) *Heterotrait-Monomethod Coefficient*: The heterotrait-monomethod (different trait, same method) coefficient is the correlation coefficient for the self-report sociability test and the self-report impulsivity test. When this coefficient is small, it provides evidence of the self-report sociability test’s divergent validity.
(d) *Heterotrait-Heteromethod Coefficient:* The heterotrait-heteromethod (different trait, different method) coefficient is the correlation coefficient for the self-report sociability test and the teacher report impulsivity test. When this coefficient is small, it provides evidence of the self-report sociability test’s divergent validity.  

2. **Factor Analysis:** Factor analysis is used for several purposes including assessing a test’s convergent and divergent validity. It involves four basic steps: (1) Administer the test that’s being validated to a sample of examinees along with tests of the same or related traits and unrelated traits. (2) Correlate all pairs of scores and list the correlation coefficients in a correlation (R) matrix. (3) Use the data in the correlation matrix to derive the initial factor matrix. (4) Rotate the initial factor matrix and interpret and name the factors. (The factor matrix is rotated because the rotated matrix produces data that’s easier to interpret than the data in the initial matrix.) For the exam, you want to be familiar with the output of the final step – the rotated factor matrix.
As an example, assume that factor analysis was used to assess the convergent and divergent validity of a newly developed test of locus of control. This involved administering the new test of locus of control (Test A) to a group of examinees along with two existing measures of locus of control (Tests B and C) and three existing measures of self-esteem, which does not correlate with locus of control (Tests D, E, and F). The factor analysis produced the following rotated factor matrix:
|             | Factor I    | Factor II   | Communality |
|-------------|-------------|-------------|-------------|
| Test A      | .80         | .10         | .65         |
| Test B      | .85         | .12         | .73         |
| Test C      | .76         | .15         | .60         |
| Test D      | .13         | .85         | .74         |
| Test E      | .14         | .76         | .60         |
| Test F      | .25         | .70         | .55         |

 
The numbers listed under Factor I and Factor II are *factor loadings*, which are correlation coefficients that indicate the correlation between each test and each identified factor. They can be interpreted by squaring them to determine how much variability in a test is explained by (shared with) variability in the factor. For example, Test A has factor loadings of .80 for Factor I and .10 for Factor II. This means that 64% (.80 squared) of variability in Test A is explained by Factor I and 1% (.10 squared) of variability in Test A is explained by Factor II. The numbers in the *communality* column indicate the amount of variability in each test that’s explained by all of the identified factors. When the communality is not listed in the table, it can be calculated by squaring and adding the factor loadings as long as the factors are orthogonal (uncorrelated) but not when they’re oblique (correlated). For example, assuming that the factors in the example are orthogonal, the communality for Test A is calculated by squaring and adding its factor loadings: .80 squared plus .10 squared is .64 plus .01, which is .65. This means that 65% of variability in Test A scores is explained by the factor analysis – i.e., by Factor I plus Factor II.

As noted above, part of the final step in factor analysis is interpreting and naming the factors. This is sometimes difficult, but it’s easy for the example because all of the tests of locus of control (Tests A, B, and C) have high correlations with Factor I and low correlations with Factor II, while all of the tests of self-esteem (Tests D, E, and F) have low correlations with Factor I and high correlations with Factor II. Therefore, the appropriate names are “locus of control” for Factor I and “self-esteem” for Factor II. With regard to Test A (the test being validated), the factor analysis provides evidence of its convergent and divergent validity because Test A has a high correlation with the factor it should correlate with (locus of control) and a low correlation with the factor it should not correlate with (self-esteem).   
