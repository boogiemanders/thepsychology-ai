# Item Analysis and Test Reliability


**Classical Test Theory and Reliability**: Classical test theory is a theory of measurement that’s used as a framework for developing and evaluating tests. It’s also known as true score test theory and is based on the assumption that obtained test scores (X) are due to a combination of true score variability (T) and measurement error (E): i.e., X = T + E. *True score variability* is the result of actual differences among examinees with regard to whatever the test is measuring. It’s assumed to be consistent, which means that an examinee’s true score will be the same regardless of which form of the test he or she takes or who scores the test. In contrast, *measurement error* is due to random factors that affect the test performance of examinees in unpredictable ways and include distractions during testing, ambiguously worded test items, and examinee fatigue.   

Test reliability refers to the extent to which a test provides consistent information. Several methods for evaluating reliability have been developed, and each is appropriate for different circumstances. Most methods produce a reliability coefficient, which is a type of correlation coefficient. Reliability coefficients range from 0 to 1.0 and are designated with the letter “r” that has a subscript containing two of the same letters or numbers (e.g., “xx”). They’re always interpreted directly as the amount of variability in obtained test scores that’s due to true score variability. For instance, if a test has a reliability coefficient of .80, this means that 80% of variability in obtained test scores is due to true score variability and the remaining 20% is due to measurement error.
The acceptable level of reliability depends on the type of test and its purpose: For example, standardized cognitive ability tests generally have higher reliability coefficients than attitude or personality tests. In addition, while a reliability coefficient of .70 or higher is considered minimally acceptable for many tests, coefficients of .90 are usually required for high-stakes tests that will be used to select employees, assign diagnoses, or make other important decisions about individuals (e.g., Kline, 2005).

**Methods for Estimating Reliability:**  There are four main methods for assessing a test’s reliability: test-retest, alternate forms, internal consistency, and inter-rater.

1. **Test-Retest Reliability:** Test-retest reliability provides information about the consistency of scores over time. It involves administering the test to a sample of examinees, re-administering the test to the same examinees at a later time, and correlating the two sets of scores. Test-retest reliability is useful for tests that are designed to measure a characteristic that’s stable over time**.**

2. **Alternate Forms Reliability:** Alternate forms reliability provides information about the consistency of scores over different forms of the test and, when the second form is administered at a later time, the consistency of scores over time. It involves administering one form of the test to a sample of examinees, administering the other form to the same examinees, and correlating the two sets of scores. Alternate forms reliability is important whenever a test has more than one form.  

3. **Internal Consistency Reliability:** Internal consistency reliability provides information on the consistency of scores over different test items and is useful for tests that are designed to measure a single content domain or aspect of behavior. It’s not useful for speed tests (tests that measure speed of performance rather than knowledge or skill level) because it tends to overestimate their reliability. For speed tests, test-retest and alternate forms reliability are appropriate.

Several methods are used to evaluate internal consistency reliability. One method is *coefficient alpha*, which is also known as Cronbach’s alpha and involves administering the test to a sample of examinees and calculating the average inter-item consistency. *Kuder-Richardson 20* (KR-20) is an alternative to coefficient alpha that can be used when test items are dichotomously scored (e.g., as correct or incorrect). Another method is *split-half reliability*, which involves administering the test to a sample of examinees, splitting the test in half (often in terms of even- and odd-numbered items), and correlating the scores on the two halves. A problem with split-half reliability is that it essentially involves calculating a reliability coefficient for two forms of the test that are half as long as the original test, and shorter tests tend to be less reliable than longer ones. Consequently, a split-half reliability coefficient underestimates a test’s reliability and is usually corrected with the *Spearman-Brown prophecy formula*, which is used to determine the effects of lengthening or shortening a test on its reliability coefficient.

4. **Inter-Rater Reliability:** Inter-rater reliability is important for measures that are subjectively scored and provides information on the consistency of scores or ratings assigned by different raters. Percent agreement and Cohen’s kappa coefficient are methods used to evaluate inter-rater reliability: *Percent agreement* can be calculated for two or more raters. A problem with this method is that it does not take chance agreement into account, which can result in an overestimate of reliability. *Cohen’s kappa coefficient* is also known as the kappa statistic and is one of several inter-rater reliability coefficients that is corrected for chance agreement between raters. It is used to assess the consistency of ratings assigned by two raters when the ratings represent a nominal scale.

The reliability of subjective ratings can be affected by *consensual observer drift*. It occurs when two or more raters communicate with each other while assigning ratings, which results in increased consistency (but often decreased accuracy) in ratings and an overestimate of inter-rater reliability. Not having raters work together, providing raters with adequate training, and regularly monitoring the accuracy of raters’ ratings are ways to eliminate or reduce consensual observer drift. 

**Factors that Affect the Reliability Coefficient:** The size of the reliability coefficient is affected by the following factors:
1. **Content Homogeneity:** Tests that are homogeneous with regard to content tend to have larger reliability coefficients than tests that are heterogeneous, and this is especially true for internal consistency reliability.

2. **Range of Scores:** Reliability coefficients are larger when test scores are unrestricted in terms of range. An unrestricted range occurs when the examinees included in the sample are heterogeneous with regard to the characteristic(s) measured by the test – i.e., when the sample includes examinees who have high, moderate, and low levels of the characteristic(s).

3. **Guessing:** Reliability coefficients are affected by the likelihood that test items can be answered correctly by guessing – i.e., the easier it is to choose the correct answer by guessing, the lower the reliability coefficient. Consequently, all other things being equal, a true/false test is likely to be less reliable than a multiple-choice test that has three or more answer choices.

**Reliability Index:** Some authors use the term “reliability index” (or “index of reliability”) interchangeably with the term “reliability coefficient.” However, others describe the reliability index as the theoretical correlation between observed test scores and true test scores and calculate it by taking the square root of the reliability coefficient (e.g., Morrow, Mood, Disch, & Kang, 2011). For example, when a test’s reliability coefficient is .81, the reliability index is the square root of .81, which is .90.

**Item Analysis:** When developing a test on the basis of classical test theory, item analysis is used to determine which items to include in the test and involves determining each item’s difficulty level and ability to discriminate between examinees who obtain high and low total test scores.

1. **Item Difficulty:** For dichotomously scored items, an item’s difficulty level (p) indicates the percentage of examinees who answered the item correctly. It’s calculated by dividing the number of examinees who answered the item correctly by the total number of examinees. As an example, when 50 of 100 examinees answered an item correctly, the item’s p value is 50/100, or .50.

The value of p ranges from 0 to 1.0, with smaller values indicating more difficult items.  For most tests, moderately difficult items are preferred (p = .30 to .70), but the optimal value is affected by the purpose of the test and the chance that items can be answered correctly by guessing: For mastery tests (tests used to identify examinees who have mastered a certain level of knowledge or skill), lower p values are preferred. For instance, when a test will be used to identify examinees who have mastered at least 20% of the information presented in an employee training program, the optimal average item difficulty level is .20. With regard to guessing, the optimal p value lies halfway between 1.0 and the probability that the item can be answered correctly by guessing. For instance, the chance of choosing the correct answer to a four-answer multiple-choice question by guessing is .25, and the optimal difficulty level for this type of item is calculated by adding 1.0 to .25 and dividing the result by 2: (1.0 + .25)/2 = 1.25/2 = .625.

2. **Item Discrimination:** The item discrimination index (D) ranges from -1.0 to +1.0 and indicates the difference between the percentage of examinees with high total test scores (often the top 27%) who answered the item correctly and the percentage of examinees with low total test scores (the bottom 27%) who answered the item correctly. As an example, when 90% of examinees in the high-scoring group and 20% of examinees in the low-scoring group answered an item correctly, the item’s D value is .90 minus .20, which is .70.  For most tests, a D value of .30 or higher is acceptable. Note that an item’s difficulty level affects its ability to discriminate, with items of moderate difficulty having higher levels of discrimination.

**Standard Error of Measurement and Confidence Intervals:** When a test’s reliability coefficient is less than 1.0, this means that an examinee’s obtained test score may or may not be his or her true score.  Consequently, obtained test scores are often interpreted in terms of a *confidence interval*, which indicates the range within which an examinee’s true score is likely to be given his or her obtained score. The *standard error of measurement* is used to construct a confidence interval, and it’s calculated by multiplying the test’s standard deviation times the square root of 1 minus the reliability coefficient. For instance, if a test has a standard deviation of 5 and a reliability coefficient of .84, its standard error of measurement equals 5 times the square root of 1 minus .84: 1 minus .84 is .16, the square root of .16 is .4, and 5 times .4 is 2. In other words, when a test’s standard deviation is 5 and its reliability coefficient is .84, its standard error of measurement is 2.

For the exam, you also want to know how to construct a 68%, 95%, and 99% confidence interval around an obtained test score:  For a 68% confidence interval, you add and subtract one standard error of measurement to and from the obtained score; for a 95% confidence interval, you add and subtract two standard errors of measurement; and for a 99% confidence interval, you add and subtract three standard errors of measurement. On the exam, a test question might state that an examinee obtained a score of 90 on a test that has a standard error of measurement of 5 and ask you to identify the 95% confidence interval for this score. To do so, you add and subtract 10 (two standard errors) to and from 90, which gives you a 95% confidence interval of 80 to 100. 

**Item Response Theory:** Item response theory (IRT) provides an alternative to classical test theory (CTT) for developing tests. An important difference between IRT and CTT is that CTT is “test based” and focuses on examinees’ total test scores, while IRT is “item based” and focuses on examinees’ responses to individual test items. As noted by Hambleton and Jones (1993), IRT’s focus on items makes it possible to determine the probability that a particular examinee will correctly answer any specific test item and to design a test with certain characteristics for a specific population (e.g., an achievement test to identify which high-achieving students should be considered for an advanced placement program). IRT also overcomes some of the limitations of CTT: For example, item parameters (e.g., discrimination, difficulty) are likely to vary from sample to sample for tests developed on the basis of CTT. In contrast, when using IRT to develop a test, sample invariant parameters are derived using mathematical techniques and a large sample size. Finally, an advantage of IRT is that it is better suited than CTT for developing computerized adaptive tests, which require (a) constructing an item bank containing a large pool of items that represent different levels of the trait measured by the test and (b) tailoring the test to each examinee by presenting items that are appropriate for the examinee’s level of the trait. 

IRT is also known as latent trait theory because it focuses on how performance on each test item relates to the latent trait measured by the test (e.g., ability, aptitude, psychopathology, personality). The relationship between each item and the latent trait is depicted with an item characteristic curve (ICC) that is derived from the responses of a large sample of examinees. The ICC for each item is plotted on a graph that contains two axes: Total test scores are used as estimates of the latent trait and are plotted from low to high on the horizontal (x) axis. The probabilities of endorsing the item or answering the item correctly are plotted from low to high on the vertical (y) axis. 

Depending on which IRT model is used, an ICC provides information on one, two, or three item parameters: (a) The *difficulty parameter* is also known as the severity parameter and location parameter. It indicates the level of the trait required for a 50% probability of endorsing the item or answering the item correctly. For example, for an ability test, the difficulty parameter indicates the level of ability required to have a 50% chance of choosing the correct answer. The location of the ICC indicates the item’s difficulty or severity: ICCs for items that are more likely to be endorsed or answered correctly are on the left side of the graph; ICCs for items that are less likely to be endorsed or answered correctly are on the right side of the graph. (b) The *discrimination parameter* indicates how well the item can discriminate between individuals with high and low levels of the trait measured by the test. It is indicated by the slope of the ICC: the steeper the slope, the better the discrimination. (c) *Probability of guessing correctly* is indicated by the point at which the ICC crosses the y-axis: The closer this point is to 0, the more difficult it is for examinees to choose the correct answer to the item just by guessing.
