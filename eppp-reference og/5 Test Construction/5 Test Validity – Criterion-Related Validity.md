# Test Validity – Criterion-Related Validity


A test’s criterion-related validity is of interest whenever scores on the test will be used to predict or estimate scores on another measure. For example, it would be important to evaluate the criterion-related validity of a job knowledge test that will be used to assist with hiring decisions by using job applicants’ scores on the test to predict their future scores on a measure of job performance. In this situation, the job knowledge test is the *predictor* and the measure of job performance is the *criterion*.

**Concurrent and Predictive Validity:** There are two types of criterion-related validity – concurrent and predictive. Both involve correlating the scores obtained by a sample of individuals on the predictor and the criterion to obtain a criterion-related validity coefficient. *Concurrent validity* is evaluated by obtaining scores on the predictor and criterion at about the same time. It’s most important when predictor scores will be used to estimate <u>current</u> status on the criterion (e.g., when a brief test of general mental ability will be used to estimate current scores on an established intelligence test). *Predictive validity* is evaluated by obtaining scores on the predictor before obtaining scores on the criterion. It’s most important when predictor scores will be used to estimate <u>future</u> status on the criterion (e.g., when a brief test of general mental ability will be used to estimate future scores on a measure of job performance).

**Interpretation of the Criterion-Related Validity Coefficient**: The criterion-related validity coefficient ranges from -1 to +1, and the closer it is to plus or minus one, the more accurate the predictor scores will be for predicting criterion scores. Like other correlation coefficients for two different measures, the criterion-related validity coefficient can be squared to determine the amount of variability in one measure that’s explained by or shared with the other measure. For example, if a job knowledge test has a criterion-related validity coefficient of .70, squaring the coefficient will indicate how much variability in job performance scores is explained by job knowledge: .70 squared is .49, which means that 49% of variability in job performance is explained by variability in job knowledge and the remaining 51% is due to other factors.

**Cross-Validation and Shrinkage:** When a predictor is being developed to predict performance on a criterion, items in the predictor that correlate most highly with the criterion are retained in the final version of the predictor. Some of the high correlations between each item and the criterion may be due to chance (random) factors. Consequently, the initial correlation coefficient for the predictor and criterion may overestimate the true correlation. For this reason, when the predictor is cross-validated (validated for a new sample), the same chance factors are not likely to be present and the correlation coefficient for the new sample is likely to be smaller than the original coefficient. In other words, the correlation coefficient is likely to “shrink” on cross-validation, and this is true for a single correlation coefficient (one predictor and one criterion) and a multiple correlation coefficient (two or more predictors and a criterion). Shrinkage is greatest when the initial sample is small and, for the multiple correlation coefficient, the number of predictors is large.
**Standard Error of Estimate and Confidence Intervals:** The scores collected in a criterion-related validity study can be used to obtain a regression equation, which is used to predict a person’s criterion score based on his or her score on the predictor.  However, whenever the criterion-related validity coefficient is not +1 or -1, there will be some error in prediction, and a person’s actual criterion score may be higher or lower than his/her predicted score. Consequently, the *standard error of estimate* is often used to construct a confidence interval around a person’s predicted criterion score. A confidence interval indicates the range within which an examinee’s true criterion score is likely to fall given his or her predicted score. 

Because the standard error of estimate is a type of standard deviation, it can be interpreted in terms of areas under the normal curve: A 68% confidence interval is constructed by adding and subtracting one standard error of estimate to and from the person’s predicted criterion score, a 95% confidence interval is constructed by adding and subtracting two standard errors, and a 99% confidence interval is constructed by adding and subtracting three standard errors.

The standard error of estimate is calculated by multiplying the criterion measure’s standard deviation times the square root of 1 minus the criterion-related validity coefficient squared. For example, when the criterion measure has a standard deviation of 5 and the criterion-related validity coefficient is .60, the standard error of estimate equals 5 times the square root of 1 minus .60 squared: .60 squared is .36, 1 minus .36 is .64, the square root of .64 is .8, and 5 times .8 is 4. When the criterion measure has a standard deviation of 5 and the criterion-related validity coefficient is .60, the standard error of estimate is 4. 
The standard error of estimate ranges from 0 to the size of the criterion measure’s standard deviation. When the validity coefficient is +1 or -1, the standard error is 0: In other words, when there’s a perfect correlation between the predictor and criterion, there’s no error in prediction. At the other extreme, when the validity coefficient is 0, the standard error of estimate equals the standard deviation of the criterion scores.

**Correction for Attenuation:** One of the factors that affects the magnitude of a criterion-related validity coefficient is the reliability of the predictor and criterion. When the predictor and/or criterion has low reliability, the magnitude of the criterion-related validity coefficient is attenuated by the effects of measurement error, which means that the validity coefficient underestimates the true relationship between the predictor and criterion. The correction for attenuation formula is used to estimate what the maximum validity coefficient would be if the predictor and/or criterion had a reliability coefficient of 1.0.
**Clinical Utility:** Even when a predictor has a large criterion-related validity coefficient, it may not have adequate clinical utility, which refers to the extent to which a test is useful for clinical purposes. Methods for evaluating a test’s clinical utility include assessing its incremental validity and its diagnostic efficiency.

1. **Incremental Validity**: Incremental validity refers to the increase in the accuracy of predictions about criterion performance that occurs by adding a new predictor to the current methods used to make predictions. It can be evaluated in one of two ways. One way is to use the Taylor-Russell tables to estimate incremental validity. This method is described in the organizational psychology content summary titled Employee Selection – Evaluation of Techniques. Another way is to conduct a criterion-related (predictive or concurrent) validity study to determine how many accurate predictions are made using the new predictor compared to the number of accurate predictions made without the new predictor.

To illustrate how a predictor’s incremental validity is evaluated using data collected in a criterion-related validity study, assume that, to improve the accuracy of our hiring decisions, we’ve developed a new *predictor* (selection test) that we want to use to predict the scores that job applicants will receive on a *criterion* (measure of job performance) three months after they’re hired. To evaluate incremental validity, we conduct a predictive validity study by administering the new predictor for six months to all job applicants. We don’t use the applicants’ scores on the new predictor to make hiring decisions but, instead, use the methods that are routinely used to hire employees. We then determine the criterion scores of 55 newly hired employees three months after they’re hired and set predictor and criterion cutoff scores to identify the number of employees in each of the following categories (high scores are at or above the cutoff, while low scores are below the cutoff): 

(a) *True positives* are the recently hired employees who obtained high scores on the predictor and criterion. If the new predictor had been used to make hiring decisions, employees in this category would have been hired because of their high scores on the predictor, and this would have been a correct decision because they received high scores on the criterion.

(b) *False positives* are the recently hired employees who obtained high scores on the predictor and low scores on the criterion. If the new predictor had been used to make hiring decisions, employees in this category would have been hired because of their high scores on the predictor,  but this would have been an incorrect decision because they received low scores on the criterion.  

(c) *True negatives* are the recently hired employees who obtained low scores on the predictor and criterion. If the new predictor had been used to make hiring decisions, employees in this category would not have been hired because of their low scores on the predictor, and this would have been a correct decision because they received low scores on the criterion.

(d) *False negatives* are the recently hired employees who obtained low scores on the predictor and high scores on the criterion. If the new predictor had been used to make hiring decisions, employees in this category would not have been hired because of their low scores on the predictor, and this would have been an incorrect decision because they received high scores on the criterion.
Assume that our predictive validity study provides the following information for 55 recently hired employees:
| True Positives<br/>High Predictor Score, High Criterion Score<br/>N = 15 | False Positives<br/>High Predictor Score, Low Criterion Score<br/>N = 5  |
|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
| False Negatives<br/>Low Predictor Score, High Criterion Score<br/>N = 10 | True Negatives<br/>Low Predictor Score, Low Criterion Score<br/>N = 25   |

 
To calculate incremental validity, the base rate is subtracted from the positive hit rate: The *base rate* is the proportion of applicants who were hired without using their scores on the new predictor and obtained high scores on the criterion. It’s calculated by dividing the number of employees who obtained high scores on the criterion (true positives plus false negatives) by the total number of employees: For our study, the base rate is (15 + 10)/55, which is .45. The *positive hit rate* is the proportion of employees who would have been hired using their scores on the new predictor and obtained high scores on the criterion. It’s calculated by dividing the number of true positives by the total number of positives: For our study, the positive hit rate is 15/20, which is .75. This means that using the new predictor to hire job applicants can be expected to increase the proportion of employees who obtain high scores on the criterion by .30 (.75 - .45), or 30%, which may or may not be acceptable depending on the circumstances.

For the exam, you want to be familiar with the definitions of incremental validity, true and false positives, and true and false negatives and know the effects of changing the predictor cutoff score on the number of true and false positives and true and false negatives. To answer a question on the latter topic, you need to know that <u>raising</u> the predictor cutoff score will result in fewer people being hired – and, therefore, fewer true and false positives and more true and false negatives. Conversely, <u>lowering</u> the predictor cutoff score will result in more people being hired – and, therefore, more true and false positives and fewer true and false negatives.

2. **Diagnostic Efficiency:** Diagnostic efficiency is also known as diagnostic validity and diagnostic accuracy and refers to the ability of a test to correctly distinguish between people who do and do not have a disorder or other characteristic. The data needed to evaluate diagnostic efficiency are collected in a criterion-related validity study in which the new measure (e.g., a screening test) is used to categorize individuals who have been identified by an established procedure as having or not having a specific diagnosis or other characteristic. As an example, assume that a new screening test designed to identify people who meet the diagnostic criteria for alcohol use disorder (AUD) was administered to a sample of 200 clinic clients who have either been found to meet or not meet the DSM diagnostic criteria for this disorder using an established procedure. The results of the screening test for this sample are listed in the following table:
|                                      | Has AUD                              | Doesn’t Have AUD                     |
|--------------------------------------|--------------------------------------|--------------------------------------|
| Screening Test Positive<br/> for AUD | True Positives N = 36                | False Positives N = 4                |
| Screening Test Negative  for AUD     | False Negatives  N = 20              | True Negatives N= 140                |

 
The following information about the screening test can be derived from these results:
*Sensitivity:* Sensitivity is the proportion of people with the disorder who are identified by the test as having the disorder. It’s calculated by dividing the true positives by the true positives plus the false negatives (TP/TP + FN). The screening test’s sensitivity equals 36/(36 + 20) = .64.

*Specificity:* Specificity is the proportion of people without the disorder who are identified by the test as not having the disorder. It’s calculated by dividing the true negatives by the true negatives plus the false positives (TN/TN + FP). The screening test’s specificity equals 140/(140 + 4) = .97.

*Hit Rate:* The hit rate is also known as overall correct classification rate and is the proportion of people who are correctly categorized by the test. It’s calculated by dividing the true positives and true negatives by the sample size (TP + TN)/(TP + TN + FP +FN).  The screening test’s hit rate is (36 + 140)/(36 + 140 + 4 + 20) = .88.

*Positive Predictive Value:* The positive predictive value indicates the probability that a person who tests positive for a disorder actually has the disorder. It’s calculated by dividing the number of true positives by the total number of positives (TP/(TP + FP). The screening test has a positive predictive value of 36/(36 + 4) = .90.

*Negative Predictive Value:* The negative predictive value indicates the probability that a person who tests negative for a disorder does not actually have the disorder. It’s calculated by dividing the number of true negatives by the total number of negatives (TN/(TN + FN). The screening test has a negative predictive value of 140/(140 + 20) = .88.

The ideal situation is for a test to have high sensitivity, specificity, hit rate and predictive values with a relatively few false positives and false negatives. Note that, while a test’s specificity and sensitivity usually do not vary from setting to setting, a test’s positive and negative predictive values are not stable but vary, depending on the prevalence of the disorder in each setting (Oleckno, 2008): As the prevalence increases, the test’s positive predictive value increases while its negative predictive value decreases and, conversely, as the prevalence decreases, the test’s positive predictive value decreases while its negative predictive value increases.

**Relationship Between Reliability and Validity:** A predictor’s reliability always places a ceiling on its validity – and, for criterion-related validity, a formula defines this relationship. The formula states that a predictor’s criterion-related validity coefficient can be no greater than its reliability index which, as noted in the description of reliability (subdomain #1), is equal to the square root of the predictor’s reliability coefficient. As an example, when a predictor has a reliability coefficient of .81, its reliability index is the square root of .81, which is .90. This means that the predictor’s criterion related validity coefficient can be no greater than .90.
