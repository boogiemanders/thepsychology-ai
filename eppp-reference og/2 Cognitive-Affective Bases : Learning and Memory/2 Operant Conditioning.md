# Operant Conditioning


Operant conditioning is useful for understanding the factors that contribute to the acquisition, maintenance, and cessation of voluntary behaviors.

**E. L. Thorndike:** Thorndike (1898) studied learning by placing hungry cats in a wooden crate (“puzzle box”) that required them to make a certain response in order to escape and obtain food placed outside the crate. He found that the cats engaged in random behaviors until they accidentally performed the behavior that opened a door and allowed them to escape. Although the cats did not immediately perform the successful behavior when they were returned to the crate, the amount of time between being placed in the crate and performing the behavior gradually decreased over subsequent trials. Based on these results, Thorndike concluded that the cats learned how to escape through a process of trial-and-error and that the likelihood that behaviors would recur depended on their consequences. According to his *law of effect,* behaviors that are followed by satisfying consequences are likely to occur again, while behaviors that are followed by dissatisfying consequences are less likely to be repeated.

**B. F. Skinner:** Skinner’s (1938) theory of operant conditioning extends Thorndike’s work and proposes that whether or not a voluntary behavior is emitted depends on how it “operates” on the environment – i.e., whether it produces reinforcement or punishment. Skinner also distinguished between positive and negative reinforcement and positive and negative punishment to indicate whether the behavior is followed by the application or removal of a stimulus: (a) *Positive reinforcement* occurs when a behavior increases or is maintained at its current level because a stimulus is applied following the behavior. Example: An employee works overtime because he’s paid extra for doing so. (b) *Negative reinforcement* occurs when a behavior increases or is maintained because a stimulus is removed following the behavior. Example: A child straightens her room because her parents stop nagging her when she does so. (c) *Positive punishment* occurs when a behavior decreases because a stimulus is applied following the behavior. Example: A child stops teasing the family dog because his parents always yell at him whenever he does so. (d) *Negative punishment* occurs when a behavior decreases because a stimulus is taken away following the behavior. Example: An adolescent stops swearing because one dollar is deducted from his weekly allowance whenever he swears.

When determining if a scenario presented in an exam question describes positive or negative reinforcement or punishment, a good strategy is to first determine whether the behavior is increasing or being maintained or is decreasing. This will indicate if the scenario is describing reinforcement or punishment. The next step is to determine if something is being applied or taken away following the behavior. When something is being applied, the scenario is describing positive reinforcement or positive punishment; when something is being taken away, it’s describing negative reinforcement or negative punishment. Alternatively, a mnemonic that helps some candidates is to associate each consequence with a different word and determine which word best fits the scenario: positive reinforcement = reward, negative reinforcement = relief, positive punishment = pain, negative punishment = loss.

**Other Operant Conditioning Terms and Procedures:** For the exam, you want to be familiar with several other terms and procedures associated with operant conditioning:

1. **Operant Extinction:** To extinguish a behavior that has been reinforced, reinforcement is withheld every time the behavior occurs. Although termination of reinforcement eventually results in a decrease or cessation of the behavior, it often initially produces a temporary increase in the behavior, which is referred to as an *extinction burst*. 

2. **Reinforcement Schedules:** The acquisition of a behavior is fastest when it’s reinforced on a *continuous schedule,* which means that the behavior is reinforced every time it occurs. However, because the continuous schedule is associated with satiation (loss of the reinforcer’s reinforcing value) and with rapid extinction of the behavior when reinforcement is stopped, the optimal procedure is to start with a continuous schedule and then switch to an *intermittent (partial) schedule* when the behavior is occurring at the desired level. There are four intermittent schedules:

(a) When using a *fixed interval (FI) schedule*, reinforcement is consistently provided after a fixed period of time regardless of how many times the behavior occurs during each interval. A rat on an FI-20 schedule is reinforced with a food pellet every 20 seconds regardless of whether it presses a lever once or multiple times during a 20-second interval. FI schedules produce a low rate of responding, with responses being made shortly before the end of each interval.

(b) When using a *variable interval (VI) schedule*, reinforcement is provided after intervals of varying and unpredictable lengths. A rat on a VI-20 schedule will be reinforced, on average, after 20 seconds, but the length of the interval varies – first after 15 seconds, then after 25 seconds, then after 20 seconds, etc. Again, as long as the rat presses the lever at least once during an interval, it will receive a food pellet at the end of the interval. VI schedules produce a steady but relatively low rate of responding.

(c) When using a *fixed ratio (FR) schedule*, reinforcement is consistently provided after a specific number of responses. A rat on an FR-10 schedule receives a food pellet after every ten lever presses. FR schedules produce a steady and relatively high rate of responding.

(d) When using a *variable ratio (VR) schedule*, reinforcement is provided after a variable number of responses. A rat on a VR-10 schedule will be reinforced, on average, after every ten lever presses, but the exact number of responses varies – first after 10 responses, then after 8 responses, then after 12 responses, etc. Of the four intermittent schedules, VR schedules produce the highest rate of responding and the greatest resistance to extinction.   

3. **Thinning:** Reducing the amount of reinforcement for a behavior is referred to as thinning. Switching from a continuous to an intermittent schedule of reinforcement or from an FR-10 to an FR-20 schedule are examples of thinning. Thinning the reinforcement schedule once a behavior reaches its desired level helps increase resistance to extinction.

4. **Behavioral Contrast:** Behavioral contrast occurs when two behaviors (Behaviors A and B) are being reinforced and the frequencies of Behavior A and Behavior B change when there is an alteration in the rate of reinforcement for one of the behaviors. There are two types of behavioral contrast: (a) When the amount of reinforcement for Behavior A is increased while the amount of reinforcement for Behavior B is unaltered, Behavior A will increase and Behavior B will decrease. This is referred to as *negative behavioral contrast*. (b) Conversely, when the amount of reinforcement for Behavior A is decreased while the amount of reinforcement for Behavior B is unaltered, Behavior A will decrease and Behavior B will increase. This is referred to as *positive behavioral contrast*. (To make this less confusing, keep in mind that whether behavioral contrast is negative or positive depends on what happens to the behavior with <u>unaltered</u> reinforcement: Negative behavioral contrast occurs when the behavior with unaltered reinforcement decreases; positive behavioral contrast occurs when the behavior with unaltered reinforcement increases.

5. **Matching Law:** According to the matching law, when two or more behaviors are concurrently reinforced on different schedules, the rate of performing each behavior is proportional to the frequency of reinforcement. As an example, when a rat is reinforced on a VI-30 schedule for pressing lever #1 and on a VI-15 schedule for pressing lever #2, the rat will press lever #2 about twice as often as it presses lever #1. The matching law also predicts that rate of responding will match the magnitude of the reinforcement. When a rat is being reinforced on a VI-30 schedule for two levers, but pressing lever #1 provides the rat with access to food for 10 seconds and pressing lever #2 provides access to food for five seconds, the rat will press lever #1 about twice as often as it presses lever #2. 

6**. Types of Positive Reinforcers:** *Primary reinforcers* (e.g., food, water) are inherently reinforcing because they satisfy needs that are related to basic survival. In contrast, *secondary reinforcers* (e.g., praise, tokens) are neutral stimuli that become reinforcing because of their association with primary reinforcers. When secondary reinforcers are associated with a variety of back-up (primary) reinforcers, they are referred to as generalized reinforcers (also known as generalized secondary reinforcers and generalized conditioned reinforcers). Money is a generalized reinforcer because it can be exchanged for a variety of back-up reinforcers. 

7.  **Superstitious Behavior:** As described by Skinner, superstitious behavior occurs when a behavior increases because it was accidentally reinforced. In one study, Skinner delivered food to pigeons every 15 seconds regardless of what they were doing. As a result, the pigeons developed an association between the behaviors they were performing just before being reinforced and performed those behaviors toward the end of subsequent 15-second intervals.  

8. **Stimulus Control:** A behavior is brought under stimulus control when it occurs in the presence of one stimulus but not another stimulus. For example, rats might learn that, when a light is blinking and they press a lever, a food pellet will be delivered; but, when the light is not blinking and they press the lever, a food pellet will not be delivered. In this situation, the blinking light is a positive discriminative stimulus (also referred to as just the discriminative stimulus or SD) because it signals that reinforcement will be delivered. In contrast, the non-blinking light is a negative discriminative stimulus (also referred to as the S-delta stimulus) because it signals that reinforcement will not be delivered. Stimulus control is an example of *two-factor learning*, which combines operant and classical conditioning: Performance of a particular behavior is due to positive reinforcement (operant conditioning). Performance of the behavior in the presence of a positive discriminative stimulus but not in the presence of a negative discriminative stimulus is the result of discrimination training (classical conditioning).  

9. **Prompts:** Prompts are cues that help initiate the performance of a behavior and include providing cues, instructions, or physical guidance. When the behavior is reinforced, prompts become associated with the reinforcement and act as positive discriminative stimuli. “Finish your homework and you can play video games” is an example of a prompt that acts as a positive discriminative stimulus. Gradually removing a prompt once the behavior is at the desired level is referred to as *fading*.

10.  **Stimulus Generalization:** Stimulus generalization in operant conditioning is the same as stimulus generalization in classical conditioning and occurs when stimuli similar to the positive discriminative (conditioned) stimulus elicit the same response. 

11. **Response Generalization:** In some situations, providing reinforcement for a specific behavior not only increases that behavior but also increases the likelihood that similar behaviors will occur, and this is referred to as response generalization.  Response generalization is occurring when a young child who’s praised for sharing a toy with another child starts sharing other toys with other children.

12.  **Escape and Avoidance Conditioning:** Escape and avoidance conditioning are applications of negative reinforcement. *Escape conditioning* occurs when a behavior occurs because it allows the individual to escape an unpleasant stimulus. As an example, a dog might escape an electric shock that’s being applied to the floor of its cage by jumping over a barrier to get to the other side of the cage where electric shock is not being applied. In this situation, the dog’s “jumping over the barrier” behavior is negative reinforced. 
*Avoidance conditioning* is the result of two-factor learning and occurs when a stimulus signals that an unpleasant stimulus is about to be applied and a behavior occurs because it allows the individual to avoid the unpleasant stimulus. A dog might learn that a blinking light signals that electric shock is about to be applied to the floor of its cage, and it jumps over the barrier as soon as the light starts to blink to avoid being shocked. In this situation, the blinking light has become associated with electric shock and is a conditioned stimulus (classical conditioning). And jumping over the barrier as soon as the light starts blinking is the result of negative reinforcement (operant conditioning).

13. **Habituation:** Habituation refers generally to the gradual decline in the frequency or magnitude of a response. In the context of operant conditioning, habituation is identified as one of the reasons why punishment does not have good long-term effects – i.e., over time, punishment tends to become less effective because the person habituates (becomes accustomed) to it. One of the dangers of punishment is that it can escalate to a dangerous (abusive) level if the person delivering the punishment continues to increase its intensity as it becomes less effective.  