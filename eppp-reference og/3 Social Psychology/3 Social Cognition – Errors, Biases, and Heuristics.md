# Social Cognition – Errors, Biases, and Heuristics

**Social Cognition – Errors, Biases, and Heuristics
Errors and Biases:** Researchers interested in social cognition distinguish between automatic and controlled cognitive processing: *Automatic processing* is fast and efficient and operates outside conscious awareness, while *controlled processing* is slower and effortful and operates with conscious awareness. Researchers have also found that automatic cognitive processing can cause errors and biases that adversely affect decisions and judgments. These include the following:

(a) The *confirmation bias* is the tendency to seek and pay attention to information that confirms our attitudes and beliefs and ignore information that refutes them. The role of the confirmation bias in maintaining one’s self-concept is addressed by *self-verification theory* (Swann, Pelham, & Krull, 1989). It predicts that, regardless of whether people have positive or negative self-concepts, they seek feedback from and prefer to spend time with others who confirm their self-concepts.

(b) *Illusory correlation* occurs when we overestimate the relationship between two variables that are not related or are only slightly related. An example is the tendency to overestimate the frequency of behaviors that are consistent with negative stereotypes of members of certain minority groups (Hamilton & Gifford, 1976). 

(c) The *base rate fallacy* is the “tendency to ignore or underuse base rate information (information about most people) and instead to be influenced by the distinctive features of the case being judged” (Baumeister & Bushman, 2013, p. 176). As an example, juries are more likely to be persuaded by anecdotal case histories than by probabilistic base-rate information (Kahneman & Tversky, 1973).

(d) The *false consensus effect* is the tendency to overestimate the extent to which other people share our opinions, values, and beliefs and has been found to affect judgments in a variety of situations. In one study, students were told they had either passed or failed a bogus social sensitivity test. When asked to estimate how other students would do, those who were told they had failed said most other students would fail the test and those who were told they had passed said most other students would pass (Alicke & Largo, 1995).

(e) The *gambler’s fallacy* occurs when people “believe that a particular chance event is affected by previous events and that chance events will ‘even out’ in the short run” (Baumeister & Bushman, 2013, p. 178).  A person is exhibiting the gambler’s fallacy when, after five coin tosses come up with heads, the person is certain that the next toss will be tails. 

(f) *Counterfactual thinking* is the tendency to imagine what might have happened but didn’t and can involve imagining either better or worse outcomes. It’s most likely to occur when the outcome is personally significant and it’s relatively easy to imagine an alternative outcome (Kahneman & Miller, 1986; Roese & Hur, 1997).

(g) *Illusory control* is also known as the illusion of control and occurs when people believe they can influence events that are outside their control. It has been used to explain superstitious behaviors that people believe will maximize their probability of success. A gambler’s belief that blowing on the dice before throwing them will help him get desired numbers and a person’s belief that she’s more likely to win the lottery if she chooses certain “lucky” numbers are examples of illusory control.

(h) The *spotlight effect* occurs when people “believe that more people take note of their actions and appearance than is actually the case” (Gilovich, Medvec, & Savitsky, 2000, p. 211). People are exhibiting the spotlight effect when they think everyone is looking at them when they arrive at a meeting late or eat dinner at a restaurant alone. The spotlight effect is similar to the imaginary audience that Elkind (1981) identified as a characteristic of adolescence and as being the result of renewed egocentrism (see the lifespan development content summary). The spotlight effect is especially common in people who have social anxiety.

(i) The *illusion of transparency* is similar to the spotlight effect in that both occur when people overestimate the extent to which other people notice them. However, the illusion of transparency applies to thoughts and feelings rather than actions and appearance and occurs when people “overestimate the extent to which others can discern their internal states” (Gilovich, Savitsky, & Medvec, 1998, p. 332). For example, Gilovich and colleagues asked subjects to drink unpleasant liquids while hiding their feelings of disgust and found that subjects overestimated how many observers would detect their true emotional reactions to the liquids. 

(j) The *hindsight bias* is also known as the “knew-it-all-along” effect. It refers to people’s judgments after an event occurs and is the tendency of people to inaccurately believe they predicted the event would occur or to overestimate the likelihood that they could have predicted that the event would occur. As an example, Blank et al. (2003) found that, after a political election, study participants’ memories of their own pre-election predictions about the percent of votes for the different political parties were closer to the actual percentages than their original predictions had been. Hindsight bias has been attributed to several factors including the need to see the world as predictable and orderly and reconstructive memory processes in which information that is already stored in memory is automatically updated with new information. 

(k) The *sunk-cost fallacy* is the tendency of people to continue investing resources (e.g., time, money) in an endeavor when they have already invested significant resources that have not produced desired outcomes and/or are not recoverable. The sunk-cost fallacy is also known as the Concorde fallacy because, in the 1960s, the French and British governments continued funding the Concorde, a supersonic plane, long after it was clear they would never recover the money that had already been spent to develop it. One explanation for the sunk-cost fallacy is that people do not want to be wasteful; another explanation is that people do not want to admit that their initial actions were a mistake. There is evidence that older adults are less susceptible than younger adults to the sunk-cost fallacy (Strough et al., 2008).

**Heuristics:** Heuristics are “mental shortcuts that provide quick estimates about the likelihood of uncertain events” (Baumeister & Bushman, 2013, p. 164). Although heuristics can be useful when it’s necessary to make quick judgments, they can lead to inaccurate conclusions. Kahneman and Tversky (1974) distinguish between representativeness, availability, and anchoring and adjustment heuristics:

(a) When using the *representativeness heuristic* to make judgments about the frequency or likelihood of an event, we ignore base rates and other important information and focus, instead, on the extent to which the event resembles a prototype (typical case). For example, assume that you’re asked to judge whether a woman is more likely to be a librarian or elementary school teacher after being told she’s friendly but a little shy, tends to speak very softly, is a conservative dresser, and keeps her house very neat. Most likely, you’ll say she’s a librarian because of her personal characteristics even though there are fewer librarians than elementary school teachers in the population.

The representativeness heuristic is used to explain the *conjunction fallacy*, which “occurs when people estimate that the odds of two uncertain events happening together are greater than the odds of either event happening alone” (Weiten, 2022, p. 279). As an example, Tversky & Kahneman (1983) presented college students with the following description: Linda, age 31, majored in philosophy, is outspoken and bright, and, as a student, was concerned about discrimination and social justice and participated in anti-nuclear demonstrations. The students were then asked to choose the statement that is more probable: (1) Linda is a bank teller or (2) Linda is a bank teller and is active in the feminist movement. Eighty-five percent of the students exhibited the conjunction fallacy and chose statement 2 even though it could not be true: There cannot be more people who are bank tellers <u>and</u> active in the feminist movement than there are people who are just bank tellers. As noted by Yoe (2019), most students in Tversky and Kahneman’s study erroneously chose statement 2 because the description of Linda is more representative of an activist than a bank teller, and people tend to rely more on representativeness than on logic or probability theory when making probability judgments. [If you are having trouble understanding why statement 1 is more probable, you’re in good company: When faced with the Linda problem, the naturalist Stephen Jay Gould said he knew that the first statement was true, but that “a little homunculus in my head continues to jump up and down, shouting at me – ‘but she can’t just be a bank teller; read the description’” (Kahneman, 2011, p. 159).]

(b) When using the *availability heuristic*, we base our judgments about the frequency or likelihood of an event on how easy it is to recall relevant examples of the event. For instance, people tend to overestimate the frequency of deaths due to shark attacks and plane crashes because, even though they’re actually uncommon, they’re highly memorable.

(c) When using the *anchoring and adjustment heuristic*, we estimate the frequency of an event or other value by beginning with a starting point and then making upward or downward adjustments. When negotiating the price of a used bicycle at a garage sale, for example, the seller’s initial price is the starting point and determines the size of the purchaser’s counteroffer.

(d) When we use the *simulation heuristic*, we judge the likelihood of an event based on how easy it is to imagine (mentally simulate) the event happening to us or others: Events that are more easily imagined are judged to be more likely to occur. The simulation heuristic differs from other heuristics because, in addition to affecting our judgment about the probability that an event will occur, it affects how we feel or think others feel about the event. For example, Kahneman and Tversky (1982) told subjects that two men arrived at the airport 30 minutes late for their flight. However, one missed his flight by 30 minutes because it left on time, while the other missed his flight by only 5 minutes because it was delayed for 25 minutes. When subjects were asked which man would feel worse about missing the flight, nearly all of the subjects said it would be the man who missed the flight by 5 minutes. According to Kahneman and Tversky, this was because it was easier for subjects to imagine that the man who missed his flight by only 5 minutes could have made the flight by doing things a little differently than it was for them to imagine that the man who missed his flight by 30 minutes could do so. In other words, when a negative event (e.g., missing a flight) can be easily undone in imagination, a person is likely to have – or predict that another person will have – a more extreme emotional reaction to that event. Note that the simulation heuristic requires counterfactual thinking, which is also known as “what if” or “if only” thinking and involves imagining alternative actions or events that would have led to different outcomes.